{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.0+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "from transformers import AutoImageProcessor, FlavaImageModel, FlavaModel, FlavaFeatureExtractor\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset a-mnist (/home/IAIS/jraghu/.cache/huggingface/datasets/gorar___a-mnist/amnist/1.1.0/49d6e25269c73523fbcc8d636818270c5604ddfdd1568ccabdcb39dc4416e954)\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.86it/s]\n"
     ]
    }
   ],
   "source": [
    "#load the MNIST dataset using the load_dataset package\n",
    "dataset = load_dataset(\"gorar/A-MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar10 (/home/IAIS/jraghu/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n",
      "100%|██████████| 2/2 [00:00<00:00,  5.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#oad CIFAR10 dataset also\n",
    "cifar10 = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "image = dataset[\"test\"][\"image\"][0]\n",
    "print(np.shape(image))\n",
    "\n",
    "#Since the MNIST dataset is 2 dimensional and FLAVA needs 3 channel input, converting the MNIST data into 3 channel dataset\n",
    "# Expand dimensions to add a channel dimension\n",
    "train_images = np.expand_dims(image, axis=-1)\n",
    "\n",
    "# Duplicate the grayscale channel into three channels\n",
    "train_images = np.repeat(train_images, 3, axis=-1)\n",
    "\n",
    "print(np.shape(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "#use this to convert into 3 channel\n",
    "t = image.convert(\"RGB\")\n",
    "print(np.shape(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n",
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['mlm_head.transform.LayerNorm.bias', 'mmm_text_head.transform.LayerNorm.bias', 'mmm_text_head.transform.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'itm_head.pooler.dense.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'mmm_text_head.transform.dense.bias', 'mlm_head.decoder.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'mmm_image_head.bias', 'image_codebook.blocks.output.conv.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'mim_head.decoder.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'mlm_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'mlm_head.transform.dense.weight', 'mmm_image_head.transform.dense.weight', 'mim_head.transform.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'mim_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'mlm_head.bias', 'mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'mmm_text_head.transform.LayerNorm.weight', 'itm_head.seq_relationship.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'mmm_image_head.transform.LayerNorm.weight', 'itm_head.seq_relationship.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.input.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'mmm_text_head.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.input.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'itm_head.pooler.dense.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'mmm_text_head.decoder.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'mmm_image_head.decoder.bias', 'mlm_head.decoder.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'mmm_text_head.decoder.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'mim_head.bias', 'mim_head.transform.dense.bias', 'mim_head.decoder.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.output.conv.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'mmm_image_head.decoder.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'mlm_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight']\n",
      "- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/transformers/models/flava/feature_extraction_flava.py:28: FutureWarning: The class FlavaFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use FlavaImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaImageModel: ['flava.text_model.encoder.layer.2.attention.attention.value.weight', 'flava.text_model.encoder.layer.2.layernorm_before.weight', 'flava.text_model.encoder.layer.7.layernorm_before.weight', 'flava.text_model.embeddings.position_ids', 'flava.text_model.encoder.layer.0.attention.attention.query.weight', 'flava.text_model.encoder.layer.4.layernorm_after.weight', 'flava.text_model.encoder.layer.1.output.dense.weight', 'flava.multimodal_model.encoder.layer.1.output.dense.bias', 'flava.text_model.encoder.layer.3.attention.attention.query.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'mmm_text_head.transform.dense.bias', 'flava.text_model.encoder.layer.11.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.0.layernorm_before.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.output.conv.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'flava.text_model.encoder.layer.9.output.dense.weight', 'flava.text_model.encoder.layer.4.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.5.output.dense.bias', 'flava.text_model.encoder.layer.6.layernorm_after.weight', 'flava.text_model.encoder.layer.4.attention.output.dense.bias', 'flava.text_model.encoder.layer.0.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.value.bias', 'flava.text_model.encoder.layer.2.attention.output.dense.bias', 'mlm_head.transform.dense.bias', 'flava.text_model.encoder.layer.8.layernorm_after.weight', 'flava.text_model.encoder.layer.7.attention.output.dense.bias', 'flava.text_model.encoder.layer.8.attention.attention.key.weight', 'flava.text_model.encoder.layer.11.attention.attention.key.bias', 'flava.text_model.encoder.layer.5.attention.attention.value.bias', 'flava.text_model.encoder.layer.9.output.dense.bias', 'flava.text_model.encoder.layer.10.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.0.layernorm_after.bias', 'flava.text_model.encoder.layer.11.layernorm_after.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'flava.text_model.encoder.layer.9.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.key.bias', 'flava.image_projection.bias', 'flava.text_model.encoder.layer.5.layernorm_after.weight', 'flava.text_model.encoder.layer.6.output.dense.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'mim_head.transform.LayerNorm.weight', 'flava.text_model.encoder.layer.1.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.key.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'flava.multimodal_model.encoder.layer.0.output.dense.weight', 'mlm_head.bias', 'flava.multimodal_model.encoder.layer.5.output.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'mmm_text_head.transform.LayerNorm.weight', 'flava.text_model.encoder.layer.0.attention.attention.key.weight', 'flava.text_to_mm_projection.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.key.weight', 'itm_head.seq_relationship.bias', 'flava.text_model.encoder.layer.11.attention.output.dense.bias', 'flava.text_model.encoder.layer.1.attention.attention.query.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'flava.text_model.encoder.layer.7.attention.attention.value.bias', 'flava.text_model.encoder.layer.3.intermediate.dense.bias', 'flava.text_model.encoder.layer.8.layernorm_after.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'flava.multimodal_model.encoder.layer.2.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.value.bias', 'flava.text_model.encoder.layer.4.attention.attention.query.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'flava.multimodal_model.encoder.layer.4.layernorm_before.bias', 'flava.text_model.encoder.layer.7.output.dense.bias', 'itm_head.pooler.dense.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'flava.text_model.encoder.layer.1.layernorm_before.weight', 'flava.text_model.encoder.layer.2.attention.output.dense.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'flava.multimodal_model.cls_token', 'flava.text_model.encoder.layer.1.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.2.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.query.weight', 'mmm_image_head.decoder.bias', 'flava.multimodal_model.encoder.layer.0.attention.attention.value.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'flava.multimodal_model.encoder.layer.3.output.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'flava.text_model.encoder.layer.8.attention.attention.key.bias', 'flava.text_model.encoder.layer.10.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.4.output.dense.weight', 'flava.text_model.encoder.layer.6.attention.attention.query.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'flava.text_model.encoder.layer.1.output.dense.bias', 'mmm_image_head.decoder.weight', 'flava.text_model.encoder.layer.2.layernorm_after.bias', 'flava.text_model.encoder.layer.6.output.dense.weight', 'flava.multimodal_model.encoder.layer.0.attention.output.dense.bias', 'flava.text_model.encoder.layer.4.attention.output.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'flava.text_model.encoder.layer.6.attention.attention.value.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'flava.multimodal_model.encoder.layer.4.attention.attention.query.bias', 'flava.text_model.encoder.layer.2.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.2.output.dense.bias', 'flava.text_model.encoder.layer.1.intermediate.dense.weight', 'flava.text_model.encoder.layer.3.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.2.attention.attention.value.weight', 'mmm_text_head.transform.dense.weight', 'flava.text_model.encoder.layer.9.intermediate.dense.weight', 'flava.text_model.encoder.layer.8.attention.attention.value.weight', 'flava.text_model.encoder.layer.9.intermediate.dense.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'flava.multimodal_model.encoder.layer.1.attention.output.dense.bias', 'flava.text_model.embeddings.LayerNorm.bias', 'mlm_head.decoder.weight', 'flava.multimodal_model.encoder.layer.0.layernorm_after.weight', 'flava.text_model.embeddings.token_type_embeddings.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'flava.text_model.encoder.layer.6.intermediate.dense.weight', 'flava.text_model.encoder.layer.9.layernorm_before.weight', 'flava.text_model.encoder.layer.2.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.layernorm_after.weight', 'flava.text_model.encoder.layer.4.attention.attention.value.bias', 'flava.text_model.encoder.layer.4.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.key.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'flava.text_model.encoder.layer.10.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.key.bias', 'flava.text_model.encoder.layer.6.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.intermediate.dense.bias', 'flava.text_model.pooler.dense.weight', 'flava.text_model.encoder.layer.6.attention.attention.query.weight', 'flava.text_model.embeddings.word_embeddings.weight', 'flava.text_model.encoder.layer.1.attention.attention.value.weight', 'flava.text_model.encoder.layer.3.output.dense.weight', 'flava.text_model.encoder.layer.7.attention.attention.value.weight', 'flava.text_model.encoder.layer.4.intermediate.dense.bias', 'flava.text_model.encoder.layer.9.attention.output.dense.bias', 'mmm_image_head.transform.dense.weight', 'mim_head.transform.dense.weight', 'flava.text_model.encoder.layer.8.attention.attention.value.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'flava.text_model.encoder.layer.10.attention.attention.key.bias', 'flava.multimodal_model.layernorm.bias', 'flava.text_model.encoder.layer.9.layernorm_after.bias', 'flava.text_model.encoder.layer.5.attention.attention.key.bias', 'flava.text_model.encoder.layer.4.layernorm_before.bias', 'flava.text_model.encoder.layer.11.output.dense.bias', 'flava.text_model.encoder.layer.1.layernorm_after.bias', 'flava.text_model.encoder.layer.1.attention.output.dense.weight', 'flava.text_model.encoder.layer.4.layernorm_before.weight', 'flava.text_model.encoder.layer.6.attention.attention.value.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'flava.text_model.encoder.layer.11.layernorm_before.weight', 'flava.text_model.encoder.layer.8.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.query.weight', 'flava.text_model.encoder.layer.4.output.dense.bias', 'flava.text_model.encoder.layer.8.attention.attention.query.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'flava.text_model.encoder.layer.6.layernorm_before.bias', 'image_codebook.blocks.input.bias', 'flava.text_model.encoder.layer.2.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.0.attention.output.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'flava.text_model.encoder.layer.9.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.0.intermediate.dense.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'flava.text_model.encoder.layer.8.output.dense.weight', 'flava.text_model.encoder.layer.10.output.dense.bias', 'flava.text_model.encoder.layer.2.output.dense.bias', 'image_codebook.blocks.input.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'flava.text_model.encoder.layer.0.layernorm_after.weight', 'flava.text_model.encoder.layer.9.attention.attention.value.bias', 'flava.text_model.encoder.layer.5.intermediate.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'flava.multimodal_model.encoder.layer.0.attention.attention.value.weight', 'flava.text_model.encoder.layer.9.layernorm_before.bias', 'flava.multimodal_model.pooler.dense.bias', 'flava.text_model.encoder.layer.10.attention.attention.key.weight', 'mim_head.transform.LayerNorm.bias', 'flava.text_model.encoder.layer.10.attention.attention.value.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'flava.text_model.encoder.layer.11.output.dense.weight', 'flava.text_model.encoder.layer.8.layernorm_before.bias', 'flava.text_model.encoder.layer.7.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.3.intermediate.dense.bias', 'mmm_text_head.decoder.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'flava.text_model.encoder.layer.3.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.key.bias', 'flava.text_model.encoder.layer.0.layernorm_after.bias', 'flava.text_model.encoder.layer.5.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.1.intermediate.dense.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.output.conv.weight', 'flava.multimodal_model.encoder.layer.1.layernorm_after.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'flava.text_model.encoder.layer.5.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.value.weight', 'flava.text_model.encoder.layer.2.layernorm_before.bias', 'flava.text_model.encoder.layer.9.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.0.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.value.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'flava.text_model.encoder.layer.6.intermediate.dense.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.0.output.dense.bias', 'flava.text_model.encoder.layer.10.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.1.attention.output.dense.weight', 'flava.text_model.encoder.layer.8.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.5.intermediate.dense.weight', 'mmm_image_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'flava.text_model.encoder.layer.11.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.0.attention.attention.query.bias', 'flava.text_model.encoder.layer.4.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.key.weight', 'flava.text_model.encoder.layer.11.intermediate.dense.bias', 'flava.text_model.encoder.layer.1.attention.attention.key.bias', 'flava.text_model.encoder.layer.0.intermediate.dense.bias', 'flava.logit_scale', 'flava.multimodal_model.encoder.layer.2.layernorm_after.weight', 'flava.text_model.layernorm.weight', 'flava.text_model.encoder.layer.8.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.5.attention.output.dense.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.query.weight', 'flava.text_model.encoder.layer.2.attention.attention.key.weight', 'flava.text_model.encoder.layer.3.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.5.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.1.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.value.weight', 'mlm_head.transform.dense.weight', 'flava.text_model.encoder.layer.5.attention.attention.value.weight', 'flava.text_model.encoder.layer.5.output.dense.weight', 'flava.text_model.encoder.layer.5.attention.output.dense.weight', 'flava.image_to_mm_projection.weight', 'flava.text_model.encoder.layer.0.output.dense.weight', 'mmm_image_head.transform.dense.bias', 'flava.multimodal_model.encoder.layer.2.intermediate.dense.bias', 'flava.text_model.encoder.layer.2.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.5.layernorm_after.bias', 'flava.text_model.encoder.layer.0.attention.attention.query.bias', 'mmm_image_head.transform.LayerNorm.weight', 'flava.image_to_mm_projection.bias', 'flava.multimodal_model.encoder.layer.5.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.4.attention.output.dense.weight', 'flava.text_model.encoder.layer.10.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.3.output.dense.bias', 'flava.multimodal_model.encoder.layer.2.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.4.layernorm_after.weight', 'flava.text_model.encoder.layer.7.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.query.weight', 'flava.text_model.encoder.layer.0.attention.attention.value.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'flava.text_model.encoder.layer.1.attention.attention.key.weight', 'flava.text_model.encoder.layer.11.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.3.attention.output.dense.weight', 'flava.text_model.embeddings.LayerNorm.weight', 'flava.text_model.encoder.layer.10.attention.attention.query.bias', 'flava.text_model.encoder.layer.1.layernorm_before.bias', 'flava.text_model.encoder.layer.7.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.4.intermediate.dense.weight', 'flava.text_model.encoder.layer.10.attention.attention.value.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'flava.multimodal_model.encoder.layer.3.intermediate.dense.weight', 'flava.text_model.encoder.layer.2.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.4.output.dense.bias', 'flava.text_projection.bias', 'flava.text_model.encoder.layer.4.attention.attention.value.weight', 'flava.text_model.encoder.layer.5.layernorm_after.bias', 'flava.text_model.encoder.layer.6.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.value.bias', 'flava.multimodal_model.encoder.layer.3.attention.output.dense.bias', 'flava.text_model.encoder.layer.1.attention.output.dense.bias', 'flava.text_model.encoder.layer.11.intermediate.dense.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'flava.text_model.encoder.layer.6.attention.output.dense.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'flava.text_to_mm_projection.weight', 'flava.text_model.encoder.layer.10.layernorm_before.bias', 'flava.multimodal_model.layernorm.weight', 'flava.text_model.encoder.layer.7.attention.output.dense.weight', 'flava.text_model.encoder.layer.7.layernorm_after.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'mim_head.bias', 'mim_head.transform.dense.bias', 'mim_head.decoder.weight', 'flava.text_model.encoder.layer.9.attention.attention.query.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'flava.text_model.encoder.layer.10.attention.output.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'flava.text_model.embeddings.position_embeddings.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.query.weight', 'flava.text_model.encoder.layer.5.attention.attention.key.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'flava.image_projection.weight', 'flava.text_model.encoder.layer.1.intermediate.dense.bias', 'flava.text_model.encoder.layer.8.output.dense.bias', 'flava.text_model.encoder.layer.7.output.dense.weight', 'flava.multimodal_model.encoder.layer.3.layernorm_after.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'flava.text_model.encoder.layer.10.layernorm_before.weight', 'flava.text_model.encoder.layer.11.attention.attention.value.bias', 'flava.text_model.encoder.layer.3.intermediate.dense.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'flava.text_model.encoder.layer.5.layernorm_before.weight', 'flava.text_model.encoder.layer.3.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.query.bias', 'mlm_head.transform.LayerNorm.bias', 'mmm_text_head.transform.LayerNorm.bias', 'flava.text_model.encoder.layer.9.attention.attention.value.weight', 'flava.text_model.encoder.layer.7.layernorm_before.bias', 'flava.text_model.encoder.layer.8.layernorm_before.weight', 'flava.text_model.encoder.layer.2.attention.attention.query.weight', 'flava.text_model.encoder.layer.7.intermediate.dense.weight', 'flava.text_model.encoder.layer.10.output.dense.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.0.layernorm_before.weight', 'flava.text_model.encoder.layer.0.layernorm_before.weight', 'flava.text_model.encoder.layer.0.attention.attention.key.bias', 'flava.text_model.encoder.layer.6.attention.attention.key.bias', 'itm_head.pooler.dense.weight', 'flava.text_model.encoder.layer.3.attention.attention.value.bias', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'flava.text_model.encoder.layer.11.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.1.output.dense.weight', 'flava.text_projection.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'mmm_image_head.bias', 'mim_head.decoder.bias', 'flava.text_model.encoder.layer.5.layernorm_before.bias', 'flava.text_model.encoder.layer.3.layernorm_before.bias', 'flava.text_model.encoder.layer.3.attention.output.dense.bias', 'flava.text_model.encoder.layer.6.layernorm_before.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'flava.text_model.encoder.layer.3.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.4.attention.attention.value.weight', 'flava.text_model.encoder.layer.8.intermediate.dense.bias', 'flava.text_model.encoder.layer.0.attention.output.dense.bias', 'flava.text_model.encoder.layer.7.attention.attention.key.weight', 'flava.text_model.encoder.layer.2.attention.attention.query.bias', 'flava.text_model.encoder.layer.4.attention.attention.query.bias', 'flava.text_model.encoder.layer.3.output.dense.bias', 'flava.multimodal_model.encoder.layer.3.layernorm_before.weight', 'flava.text_model.encoder.layer.7.attention.attention.key.bias', 'flava.text_model.encoder.layer.11.layernorm_before.bias', 'flava.text_model.encoder.layer.1.layernorm_after.weight', 'flava.multimodal_model.encoder.layer.2.attention.output.dense.bias', 'flava.text_model.encoder.layer.10.attention.output.dense.weight', 'flava.multimodal_model.encoder.layer.1.intermediate.dense.bias', 'flava.text_model.encoder.layer.4.output.dense.weight', 'flava.multimodal_model.encoder.layer.4.attention.output.dense.bias', 'flava.text_model.encoder.layer.9.attention.attention.key.weight', 'flava.text_model.encoder.layer.0.layernorm_before.bias', 'flava.multimodal_model.encoder.layer.2.layernorm_after.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'itm_head.seq_relationship.weight', 'flava.text_model.layernorm.bias', 'flava.multimodal_model.encoder.layer.5.layernorm_before.weight', 'flava.text_model.encoder.layer.2.attention.attention.key.bias', 'flava.text_model.encoder.layer.9.attention.attention.query.bias', 'flava.multimodal_model.encoder.layer.5.attention.output.dense.weight', 'flava.text_model.encoder.layer.11.layernorm_after.bias', 'flava.multimodal_model.encoder.layer.3.attention.attention.key.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'flava.text_model.encoder.layer.0.attention.attention.value.weight', 'flava.multimodal_model.encoder.layer.3.attention.attention.query.bias', 'flava.text_model.pooler.dense.bias', 'flava.text_model.encoder.layer.3.attention.output.dense.weight', 'mmm_text_head.bias', 'flava.multimodal_model.encoder.layer.2.attention.attention.key.bias', 'flava.multimodal_model.encoder.layer.1.attention.attention.key.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'flava.text_model.encoder.layer.3.attention.attention.query.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'flava.text_model.encoder.layer.4.intermediate.dense.weight', 'flava.text_model.encoder.layer.5.output.dense.bias', 'flava.multimodal_model.encoder.layer.5.attention.attention.value.bias', 'flava.text_model.encoder.layer.0.output.dense.bias', 'flava.text_model.encoder.layer.6.layernorm_after.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'mmm_text_head.decoder.bias', 'flava.multimodal_model.encoder.layer.5.intermediate.dense.bias', 'flava.multimodal_model.encoder.layer.4.layernorm_before.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'flava.multimodal_model.encoder.layer.3.layernorm_before.bias', 'flava.text_model.encoder.layer.3.layernorm_before.weight', 'flava.multimodal_model.encoder.layer.1.attention.attention.query.weight', 'flava.multimodal_model.encoder.layer.0.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.2.layernorm_before.bias', 'flava.text_model.encoder.layer.11.attention.attention.key.weight', 'flava.multimodal_model.encoder.layer.1.layernorm_after.bias', 'mlm_head.decoder.bias', 'flava.multimodal_model.encoder.layer.1.layernorm_before.bias', 'flava.text_model.encoder.layer.8.intermediate.dense.weight', 'flava.text_model.encoder.layer.5.attention.output.dense.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'flava.multimodal_model.pooler.dense.weight', 'flava.multimodal_model.encoder.layer.2.intermediate.dense.weight', 'mlm_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'flava.text_model.encoder.layer.5.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.5.layernorm_before.bias', 'flava.multimodal_model.encoder.layer.4.layernorm_after.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'flava.text_model.encoder.layer.7.attention.attention.query.bias', 'flava.text_model.encoder.layer.0.intermediate.dense.weight', 'flava.multimodal_model.encoder.layer.2.attention.attention.value.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight']\n",
      "- This IS expected if you are initializing FlavaImageModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlavaImageModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Call the main pre-trained FLAVA model API's required for building upon the pre-trained model\n",
    "flava = FlavaModel.from_pretrained(\"facebook/flava-full\").eval()\n",
    "fe = FlavaFeatureExtractor.from_pretrained(\"facebook/flava-full\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "model = FlavaImageModel.from_pretrained(\"facebook/flava-full\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out Image processor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = image_processor(train_images, return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# last_hidden_states = outputs.last_hidden_state\n",
    "# list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the MNIST dataset smaller for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = dataset[\"train\"][\"image\"][0:6000]\n",
    "train_small_label = dataset[\"train\"][\"label\"][0:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "rgb_images = []\n",
    "\n",
    "#Iterate throught he MNIST dataset to convert it into 3 channel and append them into a list\n",
    "for sample, label in zip(train_small, train_small_label):\n",
    "  # Convert to RGB\n",
    "  rgb_image = sample.convert('RGB')\n",
    "      \n",
    "  # Append the RGB image to the list\n",
    "  rgb_images.append(rgb_image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the feature extractor on these rgb_images and then get the image features on this using the ViT Base -16 transformer unit\n",
    "with torch.no_grad():\n",
    "  image_rgb = fe(rgb_images, return_tensors=\"pt\")\n",
    "  image_features = flava.get_image_features(**image_rgb)[:, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the FLAVA paper mentions that they use a L-BFGS based logistic regression classifier as a classifier head on top of the image encoder to use the mebeddings, I have implemented the same from the same source as mentioned in the paper and further ahead is the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9258333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Converting the tensor into Numpy \n",
    "features = image_features.detach().numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, train_small_label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model with L-BFGS optimization\n",
    "logistic_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 with FLAVA-- Doing the whole same procedure as done above for MNISt for CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "image_cifar = cifar10[\"train\"][\"img\"][0]\n",
    "tt = image_cifar.convert(\"RGB\")\n",
    "print(np.shape(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_cifar = cifar10[\"train\"][\"img\"][0:1000]\n",
    "train_small_label_cifar = cifar10[\"train\"][\"label\"][0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_cifar = []\n",
    "rgb_images_cifar = []\n",
    "\n",
    "for sample, label in zip(train_small_cifar, train_small_label_cifar):\n",
    "  # Convert to RGB\n",
    "  rgb_image = sample.convert('RGB')\n",
    "      \n",
    "  # Append the RGB image to the list\n",
    "  rgb_images_cifar.append(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  image_rgb_cifar = fe(rgb_images_cifar, return_tensors=\"pt\")\n",
    "  image_features = flava.get_image_features(**image_rgb_cifar)[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_cifar = image_features.detach().numpy()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_cifar, train_small_label_cifar, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model with L-BFGS optimization\n",
    "logistic_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
