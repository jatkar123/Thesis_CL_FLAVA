{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.1+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "from transformers import AutoImageProcessor, FlavaImageModel, FlavaModel, FlavaFeatureExtractor, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset a-mnist (/home/IAIS/jraghu/.cache/huggingface/datasets/gorar___a-mnist/amnist/1.1.0/49d6e25269c73523fbcc8d636818270c5604ddfdd1568ccabdcb39dc4416e954)\n",
      "100%|██████████| 2/2 [00:00<00:00, 160.73it/s]\n"
     ]
    }
   ],
   "source": [
    "#load the MNIST dataset using the load_dataset package\n",
    "dataset = load_dataset(\"gorar/A-MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar10 (/home/IAIS/jraghu/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n",
      "100%|██████████| 2/2 [00:00<00:00, 173.66it/s]\n"
     ]
    }
   ],
   "source": [
    "#oad CIFAR10 dataset also\n",
    "cifar10 = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "#Call the main pre-trained FLAVA model API's required for building upon the pre-trained model\n",
    "#flava = FlavaModel.from_pretrained(\"facebook/flava-full\").cuda().eval()\n",
    "#fe = FlavaFeatureExtractor.from_pretrained(\"facebook/flava-full\")\n",
    "\n",
    "#Optional image model calls, can be used too\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "model = FlavaImageModel.from_pretrained(\"facebook/flava-full\").cuda().eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out Image processor class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the MNIST dataset smaller for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = dataset[\"train\"][\"image\"]\n",
    "train_small_label = dataset[\"train\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "rgb_images = []\n",
    "#target=(30,30)\n",
    "\n",
    "#Iterate throught he MNIST dataset to convert it into 3 channel and append them into a list\n",
    "for sample, label in zip(train_small, train_small_label):\n",
    "  #sample =sample.resize(target)\n",
    "  \n",
    "  rgb_image = sample.convert('RGB')\n",
    "  \n",
    "  # Append the RGB image to the list\n",
    "  rgb_images.append(rgb_image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# last_hidden_states = last_hidden_states[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 337461 KiB | 337461 KiB | 337461 KiB |      0 B   |\n",
      "|       from large pool | 336384 KiB | 336384 KiB | 336384 KiB |      0 B   |\n",
      "|       from small pool |   1077 KiB |   1077 KiB |   1077 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 337461 KiB | 337461 KiB | 337461 KiB |      0 B   |\n",
      "|       from large pool | 336384 KiB | 336384 KiB | 336384 KiB |      0 B   |\n",
      "|       from small pool |   1077 KiB |   1077 KiB |   1077 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 337461 KiB | 337461 KiB | 337461 KiB |      0 B   |\n",
      "|       from large pool | 336384 KiB | 336384 KiB | 336384 KiB |      0 B   |\n",
      "|       from small pool |   1077 KiB |   1077 KiB |   1077 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 391168 KiB | 391168 KiB | 391168 KiB |      0 B   |\n",
      "|       from large pool | 389120 KiB | 389120 KiB | 389120 KiB |      0 B   |\n",
      "|       from small pool |   2048 KiB |   2048 KiB |   2048 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  53707 KiB |  56035 KiB | 264445 KiB | 210738 KiB |\n",
      "|       from large pool |  52736 KiB |  55040 KiB | 262400 KiB | 209664 KiB |\n",
      "|       from small pool |    971 KiB |   2045 KiB |   2045 KiB |   1074 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     201    |     201    |     201    |       0    |\n",
      "|       from large pool |      74    |      74    |      74    |       0    |\n",
      "|       from small pool |     127    |     127    |     127    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     201    |     201    |     201    |       0    |\n",
      "|       from large pool |      74    |      74    |      74    |       0    |\n",
      "|       from small pool |     127    |     127    |     127    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      20    |      20    |      20    |       0    |\n",
      "|       from large pool |      19    |      19    |      19    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      20    |      20    |      20    |       0    |\n",
      "|       from large pool |      19    |      19    |      19    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAtching the  dataset to get image features in batches to enable the efficeint and complete use of the GPU\n",
    "\n",
    "def batching(batch_size, rgb_images):\n",
    "    \"\"\"\n",
    "    Batch_size-> is the number o batches you want to process the dataset in; rgb_images-> the dataset list in RGB form\n",
    "    returns a list of features of lists in tensors\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the batch size\n",
    "    batch_size = 10\n",
    "\n",
    "    features_in_Tensor = []\n",
    "    # Calculate the number of batches\n",
    "    num_batches = len(rgb_images) // batch_size\n",
    "\n",
    "    # Process the images in batches\n",
    "    for batch_idx in range(num_batches):\n",
    "        # Get the start and end indices for the current batch\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        # Get the images for the current batch\n",
    "        batch_images = rgb_images[start_idx:end_idx]\n",
    "\n",
    "        # Process the images in the current batch\n",
    "        processed_images = []\n",
    "        with torch.no_grad():\n",
    "            for image in batch_images:\n",
    "                # Your processing logic here\n",
    "\n",
    "                #processed_images = fe(image, return_tensors=\"pt\").to(\"cuda\")\n",
    "                #image_features = flava.get_image_features(**processed_images)[:, 0, :]\n",
    "                processed_images = image_processor(image, return_tensors=\"pt\").to(\"cuda\")\n",
    "                image_features = model(**processed_images)\n",
    "                image_features = image_features.last_hidden_state[:, 0, :]\n",
    "                \n",
    "                features_in_Tensor.append(image_features.detach().cpu().numpy())\n",
    "        # Do something with the processed images\n",
    "        \n",
    "        \n",
    "        # Clear memory of the processed images if no longer needed\n",
    "        torch.cuda.empty_cache()\n",
    "        del processed_images, image_features\n",
    "\n",
    "    return features_in_Tensor\n",
    "\n",
    "features_in_Tensor = batching(batch_size=10, rgb_images=rgb_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the FLAVA paper mentions that they use a L-BFGS based logistic regression classifier as a classifier head on top of the image encoder to use the mebeddings, I have implemented the same from the same source as mentioned in the paper and further ahead is the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.970625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Converting the tensor into Numpy \n",
    "#features = features_in_Numpy.detach().numpy()\n",
    "features_in_Numpy = np.squeeze(features_in_Tensor, axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_in_Numpy, train_small_label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model with L-BFGS optimization\n",
    "logistic_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"training Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96000, 768) (24000, 768) (96000,) (24000,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train), np.shape(X_test), np.shape(y_train),np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.938\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing the test dataset to covert in ndarray\n",
    "test_small = dataset[\"test\"][\"image\"][0:1000]\n",
    "test_small_label = dataset[\"test\"][\"label\"][0:1000]\n",
    "\n",
    "rgb_images_test = []\n",
    "\n",
    "\n",
    "#Iterate throught he MNIST dataset to convert it into 3 channel and append them into a list\n",
    "for sample, label in zip(test_small, test_small_label):\n",
    "  \n",
    "  \n",
    "  rgb_image = sample.convert('RGB')\n",
    "  \n",
    "  # Append the RGB image to the list\n",
    "  rgb_images_test.append(rgb_image)\n",
    "\n",
    "\n",
    "features_test = batching(batch_size=10, rgb_images=rgb_images_test)\n",
    "features_test = np.squeeze(features_test, axis=1)\n",
    "# Make predictions on the unseen dataset using the same Logistic regression linear classifier\n",
    "y_pred_unseen = logistic_model.predict(features_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_small_label, y_pred_unseen)\n",
    "print(\"test set Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 with FLAVA-- Doing the whole same procedure as done above for MNISt for CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cifar = cifar10[\"train\"][\"img\"]\n",
    "train_label_cifar = cifar10[\"train\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_cifar = []\n",
    "rgb_images_cifar = []\n",
    "\n",
    "for sample, label in zip(train_cifar, train_label_cifar):\n",
    "  # Convert to RGB\n",
    "  rgb_image = sample.convert('RGB')\n",
    "      \n",
    "  # Append the RGB image to the list\n",
    "  rgb_images_cifar.append(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Calling the batching function with parameters\n",
    "features_in_Tensor = batching(batch_size=10, rgb_images=rgb_images_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Accuracy for CIFAR10: 0.962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Restructing the features in 2D format\n",
    "features_in_Numpy = np.squeeze(features_in_Tensor, axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_in_Numpy, train_label_cifar, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model with L-BFGS optimization\n",
    "logistic_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Train set Accuracy for CIFAR10:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Accuracy for CIFAR10: 0.9589\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing the test dataset to covert in ndarray\n",
    "test_cifar = cifar10[\"test\"][\"img\"]\n",
    "test_label_cifar = cifar10[\"test\"][\"label\"]\n",
    "\n",
    "rgb_images_test_cifar10 = []\n",
    "\n",
    "\n",
    "#Iterate throught he MNIST dataset to convert it into 3 channel and append them into a list\n",
    "for sample, label in zip(test_cifar, test_label_cifar):\n",
    "  \n",
    "  \n",
    "  rgb_image = sample.convert('RGB')\n",
    "  \n",
    "  # Append the RGB image to the list\n",
    "  rgb_images_test_cifar10.append(rgb_image)\n",
    "\n",
    "\n",
    "features_test = batching(batch_size=10, rgb_images=rgb_images_test_cifar10)\n",
    "features_test = np.squeeze(features_test, axis=1)\n",
    "# Make predictions on the unseen dataset using the same Logistic regression linear classifier\n",
    "y_pred_unseen = logistic_model.predict(features_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_label_cifar, y_pred_unseen)\n",
    "print(\"Test set Accuracy for CIFAR10:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
