{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.0+cu117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "from transformers import AutoImageProcessor, FlavaImageModel, FlavaModel, FlavaFeatureExtractor, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset a-mnist (/home/IAIS/jraghu/.cache/huggingface/datasets/gorar___a-mnist/amnist/1.1.0/49d6e25269c73523fbcc8d636818270c5604ddfdd1568ccabdcb39dc4416e954)\n",
      "100%|██████████| 2/2 [00:00<00:00, 160.06it/s]\n"
     ]
    }
   ],
   "source": [
    "#load the MNIST dataset using the load_dataset package\n",
    "dataset = load_dataset(\"gorar/A-MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar10 (/home/IAIS/jraghu/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n",
      "100%|██████████| 2/2 [00:00<00:00, 199.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#oad CIFAR10 dataset also\n",
    "cifar10 = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(30, 30)\n",
      "(30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "image = dataset[\"test\"][\"image\"][0]\n",
    "print(np.shape(image))\n",
    "target=(30,30)\n",
    "image = image.resize(target)\n",
    "print(np.shape(image))\n",
    "#Since the MNIST dataset is 2 dimensional and FLAVA needs 3 channel input, converting the MNIST data into 3 channel dataset\n",
    "# Expand dimensions to add a channel dimension\n",
    "train_images = np.expand_dims(image, axis=-1)\n",
    "\n",
    "# Duplicate the grayscale channel into three channels\n",
    "train_images = np.repeat(train_images, 3, axis=-1)\n",
    "\n",
    "print(np.shape(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 3)\n"
     ]
    }
   ],
   "source": [
    "#use this to convert into 3 channel\n",
    "t = image.convert(\"RGB\")\n",
    "print(np.shape(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n",
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/transformers/models/flava/feature_extraction_flava.py:28: FutureWarning: The class FlavaFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use FlavaImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "#Call the main pre-trained FLAVA model API's required for building upon the pre-trained model\n",
    "flava = FlavaModel.from_pretrained(\"facebook/flava-full\").cuda().eval()\n",
    "fe = FlavaFeatureExtractor.from_pretrained(\"facebook/flava-full\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "model = FlavaImageModel.from_pretrained(\"facebook/flava-full\")#.cuda().eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out Image processor class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the MNIST dataset smaller for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = dataset[\"train\"][\"image\"]\n",
    "train_small_label = dataset[\"train\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "rgb_images = []\n",
    "target=(30,30)\n",
    "\n",
    "#Iterate throught he MNIST dataset to convert it into 3 channel and append them into a list\n",
    "for sample, label in zip(train_small, train_small_label):\n",
    "  sample =sample.resize(target)\n",
    "  \n",
    "  rgb_image = sample.convert('RGB')\n",
    "  \n",
    "  # Append the RGB image to the list\n",
    "  rgb_images.append(rgb_image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the GPU (if not already on the GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "initial_memory = torch.cuda.memory_allocated(device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 67.29 GiB (GPU 0; 31.74 GiB total capacity; 1.22 GiB already allocated; 28.07 GiB free; 1.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[39m=\u001b[39m image_processor(rgb_images, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/feature_extraction_utils.py:224\u001b[0m, in \u001b[0;36mBatchFeature.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    221\u001b[0m     \u001b[39m# check if v is a floating point\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mis_floating_point(v):\n\u001b[1;32m    223\u001b[0m         \u001b[39m# cast and send to device\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m         new_data[k] \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    225\u001b[0m     \u001b[39melif\u001b[39;00m device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m         new_data[k] \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 67.29 GiB (GPU 0; 31.74 GiB total capacity; 1.22 GiB already allocated; 28.07 GiB free; 1.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#inputs = image_processor(rgb_images, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "\n",
    "# last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# last_hidden_states = last_hidden_states[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the feature extractor on these rgb_images and then get the image features on this using the ViT Base -16 transformer unit\n",
    "with torch.no_grad():\n",
    "  image_rgb = fe(rgb_images, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "  #image_features = flava.get_image_features(**image_rgb)[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input image size (224*224) doesn't match model (30*30).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_features \u001b[39m=\u001b[39m flava\u001b[39m.\u001b[39;49mget_image_features(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mimage_rgb)[:, \u001b[39m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/flava/modeling_flava.py:1321\u001b[0m, in \u001b[0;36mFlavaModel.get_image_features\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1285\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(FLAVA_IMAGE_INPUTS_DOCSTRING\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mbatch_size, image_num_patches\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_image_features\u001b[39m(\n\u001b[1;32m   1287\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     return_dict: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1296\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor:\n\u001b[1;32m   1297\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[39m        image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         _CHECKPOINT_FOR_DOC\n\u001b[1;32m   1320\u001b[0m     )\n\u001b[0;32m-> 1321\u001b[0m     image_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_model(\n\u001b[1;32m   1322\u001b[0m         pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   1323\u001b[0m         bool_masked_pos\u001b[39m=\u001b[39;49mbool_masked_pos,\n\u001b[1;32m   1324\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1325\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1326\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1327\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1328\u001b[0m         interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding,\n\u001b[1;32m   1329\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1330\u001b[0m     )\n\u001b[1;32m   1332\u001b[0m     pooled_output \u001b[39m=\u001b[39m image_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# last_hidden_state\u001b[39;00m\n\u001b[1;32m   1333\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_projection(pooled_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/flava/modeling_flava.py:959\u001b[0m, in \u001b[0;36mFlavaImageModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    954\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    956\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    957\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 959\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    960\u001b[0m     pixel_values, bool_masked_pos\u001b[39m=\u001b[39;49mbool_masked_pos, interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding\n\u001b[1;32m    961\u001b[0m )\n\u001b[1;32m    963\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    964\u001b[0m     embedding_output,\n\u001b[1;32m    965\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    969\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    970\u001b[0m )\n\u001b[1;32m    971\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/flava/modeling_flava.py:310\u001b[0m, in \u001b[0;36mFlavaImageEmbeddings.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    304\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    305\u001b[0m     pixel_values: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    306\u001b[0m     bool_masked_pos: Optional[torch\u001b[39m.\u001b[39mBoolTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    307\u001b[0m     interpolate_pos_encoding: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    309\u001b[0m     batch_size, num_channels, height, width \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 310\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embeddings(pixel_values, interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding)\n\u001b[1;32m    312\u001b[0m     batch_size, seq_len, _ \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39msize()\n\u001b[1;32m    313\u001b[0m     \u001b[39mif\u001b[39;00m bool_masked_pos \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/flava/modeling_flava.py:367\u001b[0m, in \u001b[0;36mPatchEmbeddings.forward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m interpolate_pos_encoding:\n\u001b[1;32m    366\u001b[0m     \u001b[39mif\u001b[39;00m height \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m0\u001b[39m] \u001b[39mor\u001b[39;00m width \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 367\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    368\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput image size (\u001b[39m\u001b[39m{\u001b[39;00mheight\u001b[39m}\u001b[39;00m\u001b[39m*\u001b[39m\u001b[39m{\u001b[39;00mwidth\u001b[39m}\u001b[39;00m\u001b[39m) doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m*\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m         )\n\u001b[1;32m    371\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection(pixel_values)\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    372\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mValueError\u001b[0m: Input image size (224*224) doesn't match model (30*30)."
     ]
    }
   ],
   "source": [
    "image_features = flava.get_image_features(**image_rgb)[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    930 MiB |   1633 MiB |   4648 GiB |   4648 GiB |\n",
      "|       from large pool |    928 MiB |    934 MiB |   2307 GiB |   2306 GiB |\n",
      "|       from small pool |      2 MiB |    700 MiB |   2341 GiB |   2341 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    930 MiB |   1633 MiB |   4648 GiB |   4648 GiB |\n",
      "|       from large pool |    928 MiB |    934 MiB |   2307 GiB |   2306 GiB |\n",
      "|       from small pool |      2 MiB |    700 MiB |   2341 GiB |   2341 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |    929 MiB |   1631 MiB |   4485 GiB |   4484 GiB |\n",
      "|       from large pool |    927 MiB |    932 MiB |   2144 GiB |   2143 GiB |\n",
      "|       from small pool |      2 MiB |    700 MiB |   2341 GiB |   2341 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   1836 MiB |   1842 MiB |  11638 MiB |   9802 MiB |\n",
      "|       from large pool |   1030 MiB |   1030 MiB |   1030 MiB |      0 MiB |\n",
      "|       from small pool |    806 MiB |    812 MiB |  10608 MiB |   9802 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 105992 KiB | 221232 KiB |   5056 GiB |   5055 GiB |\n",
      "|       from large pool | 104320 KiB | 106240 KiB |   2307 GiB |   2307 GiB |\n",
      "|       from small pool |   1672 KiB | 116912 KiB |   2748 GiB |   2748 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     516    |    1727    |    5242 K  |    5241 K  |\n",
      "|       from large pool |     191    |     194    |    1123 K  |    1123 K  |\n",
      "|       from small pool |     325    |    1534    |    4118 K  |    4118 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     516    |    1727    |    5242 K  |    5241 K  |\n",
      "|       from large pool |     191    |     194    |    1123 K  |    1123 K  |\n",
      "|       from small pool |     325    |    1534    |    4118 K  |    4118 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     451    |     454    |    5352    |    4901    |\n",
      "|       from large pool |      48    |      48    |      48    |       0    |\n",
      "|       from small pool |     403    |     406    |    5304    |    4901    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      48    |     456    |    2995 K  |    2994 K  |\n",
      "|       from large pool |      46    |      47    |     936 K  |     936 K  |\n",
      "|       from small pool |       2    |     410    |    2058 K  |    2058 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 12\n",
    "\n",
    "features_in_Tensor = []\n",
    "# Calculate the number of batches\n",
    "num_batches = len(rgb_images) // batch_size\n",
    "\n",
    "# Process the images in batches\n",
    "for batch_idx in range(num_batches):\n",
    "    # Get the start and end indices for the current batch\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "\n",
    "    # Get the images for the current batch\n",
    "    batch_images = rgb_images[start_idx:end_idx]\n",
    "\n",
    "    # Process the images in the current batch\n",
    "    processed_images = []\n",
    "    with torch.no_grad():\n",
    "        for image in batch_images:\n",
    "            # Your processing logic here\n",
    "\n",
    "            processed_images = fe(image, return_tensors=\"pt\").to(\"cuda\")\n",
    "            image_features = flava.get_image_features(**processed_images)[:, 0, :]\n",
    "\n",
    "            features_in_Tensor.append(image_features.detach().cpu().numpy())\n",
    "    # Do something with the processed images\n",
    "    # For example, perform further computations or analysis\n",
    "    \n",
    "    # Clear memory of the processed images if no longer needed\n",
    "    torch.cuda.empty_cache()\n",
    "    del processed_images, image_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the FLAVA paper mentions that they use a L-BFGS based logistic regression classifier as a classifier head on top of the image encoder to use the mebeddings, I have implemented the same from the same source as mentioned in the paper and further ahead is the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9549583333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Converting the tensor into Numpy \n",
    "#features = features_in_Numpy.detach().numpy()\n",
    "features_in_Numpy = np.squeeze(features_in_Tensor, axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_in_Numpy, train_small_label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model with L-BFGS optimization\n",
    "logistic_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96000, 768) (24000, 768) (96000,) (24000,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train), np.shape(X_test), np.shape(y_train),np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28) (10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing the test dataset to covert in ndarray\n",
    "test_small = dataset[\"test\"][\"image\"]\n",
    "test_small_label = dataset[\"test\"][\"label\"]\n",
    "\n",
    "image_unseen = []\n",
    "\n",
    "# Convert each image in the list to a numpy array\n",
    "for image in test_small:\n",
    "    # Convert the PIL image to a numpy array\n",
    "    img_array = np.array(image)\n",
    "\n",
    "    \n",
    "    # Append the array to the 'image_arrays' list\n",
    "    image_unseen.append(img_array)\n",
    "\n",
    "print(np.shape(image_unseen), np.shape(test_small_label))\n",
    "image_array = np.array(image_unseen)\n",
    "image_array_flattened = image_array.reshape(10000, -1)\n",
    "np.shape(image_array_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 784 features, but LogisticRegression is expecting 768 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Testing on completely unseen MNIST data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39m# Make predictions on the unseen dataset\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m y_pred_unseen \u001b[39m=\u001b[39m logistic_model\u001b[39m.\u001b[39;49mpredict(image_array_flattened)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m      8\u001b[0m accuracy \u001b[39m=\u001b[39m accuracy_score(test_small_label, y_pred_unseen)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/linear_model/_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mPredict class labels for samples in X.\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    418\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 419\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    421\u001b[0m     indices \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/linear_model/_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    397\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    398\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[0;32m--> 400\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    401\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[1;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39mreshape(scores, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 588\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    590\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    390\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 784 features, but LogisticRegression is expecting 768 features as input."
     ]
    }
   ],
   "source": [
    "#Testing on completely unseen MNIST data\n",
    "\n",
    "# Make predictions on the unseen dataset\n",
    "y_pred_unseen = logistic_model.predict(image_array_flattened)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_small_label, y_pred_unseen)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 with FLAVA-- Doing the whole same procedure as done above for MNISt for CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "image_cifar = cifar10[\"train\"][\"img\"][0]\n",
    "tt = image_cifar.convert(\"RGB\")\n",
    "print(np.shape(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cifar = cifar10[\"train\"][\"img\"]\n",
    "train_label_cifar = cifar10[\"train\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_cifar = []\n",
    "rgb_images_cifar = []\n",
    "\n",
    "for sample, label in zip(train_cifar, train_label_cifar):\n",
    "  # Convert to RGB\n",
    "  rgb_image = sample.convert('RGB')\n",
    "      \n",
    "  # Append the RGB image to the list\n",
    "  rgb_images_cifar.append(rgb_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BAtching the Cifar10 dataset to get image features in batches to enable the efficeint and complete use of the GPU\n",
    "\n",
    "def batching(batch_size, rgb_images):\n",
    "    \"\"\"\n",
    "    Batch_size-> is the number o batches you want to process the dataset in; rgb_images-> the dataset list in RGB form\n",
    "    returns a list of features of lists in tensors\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the batch size\n",
    "    batch_size = 10\n",
    "\n",
    "    features_in_Tensor = []\n",
    "    # Calculate the number of batches\n",
    "    num_batches = len(rgb_images) // batch_size\n",
    "\n",
    "    # Process the images in batches\n",
    "    for batch_idx in range(num_batches):\n",
    "        # Get the start and end indices for the current batch\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        # Get the images for the current batch\n",
    "        batch_images = rgb_images[start_idx:end_idx]\n",
    "\n",
    "        # Process the images in the current batch\n",
    "        processed_images = []\n",
    "        with torch.no_grad():\n",
    "            for image in batch_images:\n",
    "                # Your processing logic here\n",
    "\n",
    "                processed_images = fe(image, return_tensors=\"pt\").to(\"cuda\")\n",
    "                image_features = flava.get_image_features(**processed_images)[:, 0, :]\n",
    "\n",
    "                features_in_Tensor.append(image_features.detach().cpu().numpy())\n",
    "        # Do something with the processed images\n",
    "        \n",
    "        \n",
    "        # Clear memory of the processed images if no longer needed\n",
    "        torch.cuda.empty_cache()\n",
    "        del processed_images, image_features\n",
    "\n",
    "    return features_in_Tensor\n",
    "\n",
    "\n",
    "#Calling the batching function with parameters\n",
    "features_in_Tensor = batching(batch_size=10, rgb_images=rgb_images_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Restructing the features in 2D format\n",
    "features_in_Numpy = np.squeeze(features_in_Tensor, axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_in_Numpy, train_label_cifar, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model with L-BFGS optimization\n",
    "logistic_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3) (10000,)\n",
      "(10000, 3072)\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing the test dataset to covert in ndarray\n",
    "test_cifar = cifar10[\"test\"][\"img\"]\n",
    "test_label_cifar = cifar10[\"test\"][\"label\"]\n",
    "\n",
    "image_unseen = []\n",
    "\n",
    "# Convert each image in the list to a numpy array\n",
    "for image in test_cifar:\n",
    "    # Convert the PIL image to a numpy array\n",
    "    img_array = np.array(image)\n",
    "\n",
    "    \n",
    "    # Append the array to the 'image_arrays' list\n",
    "    image_unseen.append(img_array)\n",
    "\n",
    "print(np.shape(image_unseen), np.shape(test_label_cifar))\n",
    "image_array = np.array(image_unseen)\n",
    "image_array_flattened = image_array.reshape(10000, -1)\n",
    "print(np.shape(image_array_flattened))\n",
    "image_array_reshaped = np.concatenate((image_array_flattened[:, :1024], image_array_flattened[:, 1024:2048], image_array_flattened[:, 2048:]), axis=1)\n",
    "print(np.shape(image_array_reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on completely unseen CIFAR data\n",
    "\n",
    "\n",
    "# Make predictions on the unseen dataset\n",
    "y_pred_unseen = logistic_model.predict(image_array_flattened)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_small_label, y_pred_unseen)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
