{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "from transformers import AutoProcessor, FlavaModel, FlavaImageModel, AutoImageProcessor, FlavaFeatureExtractor\n",
    "import numpy as np\n",
    "from torchmultimodal.models.flava.model import flava_model_for_classification\n",
    "from transformers import FlavaProcessor, FlavaForPreTraining\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available on this system.\n"
     ]
    }
   ],
   "source": [
    "#Checking for GPU and settingt he device as GPU if avaialble\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available on this system.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjatinkarthik89\u001b[0m (\u001b[33mcontinual_learning_flava\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a login artifact into wnadb logger to start logging of metrics\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
    "from avalanche.benchmarks.datasets import MNIST, CUB200\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "resize_transform = transforms.Resize((224, 224))\n",
    "grayscale_transform = transforms.Grayscale(num_output_channels=3)\n",
    "train_transform = Compose([\n",
    "    resize_transform,\n",
    "    grayscale_transform,\n",
    "    ToTensor()\n",
    "    \n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    resize_transform,\n",
    "    grayscale_transform,\n",
    "    ToTensor()\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "cub_train = CUB200(\n",
    "    './data/cub200', train=True, download=True, transform=train_transform\n",
    ")\n",
    "\n",
    "cub_test = CUB200(\n",
    "    './data/cub200', train=False, download=True,  transform=test_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classsic class incremental creation from MNIST dataset\n",
    "# scenario_CIL = nc_benchmark(\n",
    "#     cub_train, cub_test, n_experiences=10, shuffle=True, seed=1234,\n",
    "#     task_labels=True\n",
    "# )\n",
    "\n",
    "\n",
    "#Classsic Domain incremental creation from MNIST dataset\n",
    "scenario_DIL = ni_benchmark(\n",
    "    cub_train, cub_test, n_experiences=3, shuffle=False, seed=42,\n",
    "    balance_experiences=True \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing 10 out of 12 layers of the image encoder part of FLAVA to reduce the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flava_custom_with_limited_freezing(torch.nn.Module):\n",
    "    def __init__(self, num_classes, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "        self.model_custom = FlavaImageModel.from_pretrained(\"facebook/flava-full\").eval().to(\"cuda\")\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "                nn.Linear(197*768, 1000),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1000, 500),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(500, 200),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, list):\n",
    "            x = list(x.cuda())\n",
    "        \n",
    "        # Assuming you want to freeze the first 9 layers\n",
    "        # for param in self.model_custom.encoder.layer[:9].parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        \n",
    "        # child_counter = 0\n",
    "        # for child in self.model_custom.children():\n",
    "        #     children_of_child_counter = 0\n",
    "        #     #print(\"The children no. \", child_counter, \" is \", child )\n",
    "        #     if child_counter == 1:\n",
    "        #         for sub_child in child.modules():\n",
    "        #             #print(\"The sub child no. \", children_of_child_counter, \" is \", sub_child )\n",
    "        #             children_of_child_counter +=1\n",
    "        #             if children_of_child_counter < 182: #182 becuase that is where top 9 layers of FLAVA encoder is numbered as children\n",
    "        #                 for param in sub_child.parameters():\n",
    "        #                     param.requires_grad = False\n",
    "        #             else:\n",
    "        #                 pass\n",
    "        #     else:\n",
    "        #         pass\n",
    "        #     child_counter +=1\n",
    "\n",
    "        child_counter = 0\n",
    "        for child in self.model_custom.children():\n",
    "            #print(\"The children no. \", child_counter, \" is \", child )\n",
    "            if child_counter < 1: #2 for 2 child frozen which gives best results as of now\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            child_counter += 1\n",
    "\n",
    "        #with torch.no_grad():\n",
    "        inputs = self.image_processor(images=x, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model_custom(**inputs)\n",
    "            \n",
    "        #specific_layer_frozen = outputs.hidden_states[1]\n",
    "        #features = specific_layer_frozen[:, 0:100, :].view(specific_layer_frozen[:, 0:100, :].size(0), -1)  # Flatten spatial dimensions\n",
    "        features = outputs.last_hidden_state.view(outputs.last_hidden_state.size(0), -1)  # Flatten spatial dimensions\n",
    "        logits = self.mlp(features)  \n",
    "\n",
    "        \n",
    "        #trial = outputs.last_hidden_state\n",
    "        #print(\"hidden states \", list(trial.shape), \"feature size \", list(features.shape))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "flava_spec_frozen = flava_custom_with_limited_freezing(num_classes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4jbu80nd) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Flava 10 child layers frozen</strong> at: <a href='https://wandb.ai/continual_learning_flava/FLAVA%20frozen%20model%20with%20CUB200%20Experience%20Replay%20CL/runs/4jbu80nd' target=\"_blank\">https://wandb.ai/continual_learning_flava/FLAVA%20frozen%20model%20with%20CUB200%20Experience%20Replay%20CL/runs/4jbu80nd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230901_105202-4jbu80nd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4jbu80nd). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/IAIS/jraghu/bentoml/Thesis/Custom_CL/avalanche_custom/Experiment_permutations/wandb/run-20230901_105241-3395chof</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/continual_learning_flava/FLAVA%20frozen%20model%20with%20CUB200%20Experience%20Replay%20CL/runs/3395chof' target=\"_blank\">Flava 10 child layers frozen</a></strong> to <a href='https://wandb.ai/continual_learning_flava/FLAVA%20frozen%20model%20with%20CUB200%20Experience%20Replay%20CL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/continual_learning_flava/FLAVA%20frozen%20model%20with%20CUB200%20Experience%20Replay%20CL' target=\"_blank\">https://wandb.ai/continual_learning_flava/FLAVA%20frozen%20model%20with%20CUB200%20Experience%20Replay%20CL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/continual_learning_flava/FLAVA%20frozen%20model%20with%20CUB200%20Experience%20Replay%20CL/runs/3395chof' target=\"_blank\">https://wandb.ai/continual_learning_flava/FLAVA%20frozen%20model%20with%20CUB200%20Experience%20Replay%20CL/runs/3395chof</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n",
      "-- >> Start of training phase << --\n",
      "  0%|          | 0/20 [00:51<?, ?it/s].68s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/wandb/sdk/wandb_run.py:2077: UserWarning: Run (4jbu80nd) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent Classes: \u001b[39m\u001b[39m\"\u001b[39m, experience\u001b[39m.\u001b[39mclasses_in_this_experience)\n\u001b[1;32m     70\u001b[0m \u001b[39m# train returns a dictionary which contains all the metric values\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m res \u001b[39m=\u001b[39m cl_strategy_DIL\u001b[39m.\u001b[39;49mtrain(experience)\n\u001b[1;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining completed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mComputing accuracy on the whole test set\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/avalanche/training/templates/base_sgd.py:146\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m           experiences: Union[CLExperience,\n\u001b[1;32m    141\u001b[0m                              ExpSequence],\n\u001b[1;32m    142\u001b[0m           eval_streams: Optional[Sequence[Union[CLExperience,\n\u001b[1;32m    143\u001b[0m                                                 ExpSequence]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    144\u001b[0m           \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 146\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtrain(experiences, eval_streams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_last_metrics()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/avalanche/training/templates/base.py:116\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperience \u001b[39min\u001b[39;00m experiences:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_before_training_exp(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_exp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperience, eval_streams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training_exp(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/avalanche/training/templates/base_sgd.py:264\u001b[0m, in \u001b[0;36mBaseSGDTemplate._train_exp\u001b[0;34m(self, experience, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_epoch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training_epoch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/avalanche/training/templates/update_type/sgd_update.py:21\u001b[0m, in \u001b[0;36mSGDUpdate.training_epoch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39m# Forward\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_before_forward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmb_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward()\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_forward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Loss & Backward\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/avalanche/training/templates/problem_type/supervised_problem.py:27\u001b[0m, in \u001b[0;36mSupervisedProblem.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     26\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the model's output given the current mini-batch.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[39mreturn\u001b[39;00m avalanche_forward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmb_x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmb_task_id)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/avalanche/models/utils.py:13\u001b[0m, in \u001b[0;36mavalanche_forward\u001b[0;34m(model, x, task_labels)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m model(x, task_labels)\n\u001b[1;32m     12\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# no task labels\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mreturn\u001b[39;00m model(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m, in \u001b[0;36mflava_custom_with_limited_freezing.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m     child_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[39m#with torch.no_grad():\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_processor(images\u001b[39m=\u001b[39;49mx, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_custom(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[1;32m     48\u001b[0m \u001b[39m#specific_layer_frozen = outputs.hidden_states[1]\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m#features = specific_layer_frozen[:, 0:100, :].view(specific_layer_frozen[:, 0:100, :].size(0), -1)  # Flatten spatial dimensions\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/image_processing_utils.py:494\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, images, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchFeature:\n\u001b[1;32m    493\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(images, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/flava/image_processing_flava.py:664\u001b[0m, in \u001b[0;36mFlavaImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_image_mask, input_size_patches, total_mask_patches, mask_group_min_patches, mask_group_max_patches, mask_group_min_aspect_ratio, mask_group_max_aspect_ratio, return_codebook_pixels, codebook_do_resize, codebook_size, codebook_resample, codebook_do_center_crop, codebook_crop_size, codebook_do_rescale, codebook_rescale_factor, codebook_do_map_pixels, codebook_do_normalize, codebook_image_mean, codebook_image_std, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid_images(images):\n\u001b[1;32m    659\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    660\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m     )\n\u001b[0;32m--> 664\u001b[0m processed_images \u001b[39m=\u001b[39m [\n\u001b[1;32m    665\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_image(\n\u001b[1;32m    666\u001b[0m         image\u001b[39m=\u001b[39mimg,\n\u001b[1;32m    667\u001b[0m         do_resize\u001b[39m=\u001b[39mdo_resize,\n\u001b[1;32m    668\u001b[0m         size\u001b[39m=\u001b[39msize,\n\u001b[1;32m    669\u001b[0m         resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m    670\u001b[0m         do_center_crop\u001b[39m=\u001b[39mdo_center_crop,\n\u001b[1;32m    671\u001b[0m         crop_size\u001b[39m=\u001b[39mcrop_size,\n\u001b[1;32m    672\u001b[0m         do_rescale\u001b[39m=\u001b[39mdo_rescale,\n\u001b[1;32m    673\u001b[0m         rescale_factor\u001b[39m=\u001b[39mrescale_factor,\n\u001b[1;32m    674\u001b[0m         do_normalize\u001b[39m=\u001b[39mdo_normalize,\n\u001b[1;32m    675\u001b[0m         image_mean\u001b[39m=\u001b[39mimage_mean,\n\u001b[1;32m    676\u001b[0m         image_std\u001b[39m=\u001b[39mimage_std,\n\u001b[1;32m    677\u001b[0m         do_map_pixels\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    678\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    680\u001b[0m     \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images\n\u001b[1;32m    681\u001b[0m ]\n\u001b[1;32m    682\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: processed_images}\n\u001b[1;32m    684\u001b[0m \u001b[39mif\u001b[39;00m return_codebook_pixels:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/flava/image_processing_flava.py:665\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid_images(images):\n\u001b[1;32m    659\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    660\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    662\u001b[0m     )\n\u001b[1;32m    664\u001b[0m processed_images \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 665\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preprocess_image(\n\u001b[1;32m    666\u001b[0m         image\u001b[39m=\u001b[39;49mimg,\n\u001b[1;32m    667\u001b[0m         do_resize\u001b[39m=\u001b[39;49mdo_resize,\n\u001b[1;32m    668\u001b[0m         size\u001b[39m=\u001b[39;49msize,\n\u001b[1;32m    669\u001b[0m         resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m    670\u001b[0m         do_center_crop\u001b[39m=\u001b[39;49mdo_center_crop,\n\u001b[1;32m    671\u001b[0m         crop_size\u001b[39m=\u001b[39;49mcrop_size,\n\u001b[1;32m    672\u001b[0m         do_rescale\u001b[39m=\u001b[39;49mdo_rescale,\n\u001b[1;32m    673\u001b[0m         rescale_factor\u001b[39m=\u001b[39;49mrescale_factor,\n\u001b[1;32m    674\u001b[0m         do_normalize\u001b[39m=\u001b[39;49mdo_normalize,\n\u001b[1;32m    675\u001b[0m         image_mean\u001b[39m=\u001b[39;49mimage_mean,\n\u001b[1;32m    676\u001b[0m         image_std\u001b[39m=\u001b[39;49mimage_std,\n\u001b[1;32m    677\u001b[0m         do_map_pixels\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    678\u001b[0m         data_format\u001b[39m=\u001b[39;49mdata_format,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    680\u001b[0m     \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images\n\u001b[1;32m    681\u001b[0m ]\n\u001b[1;32m    682\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m: processed_images}\n\u001b[1;32m    684\u001b[0m \u001b[39mif\u001b[39;00m return_codebook_pixels:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/flava/image_processing_flava.py:459\u001b[0m, in \u001b[0;36mFlavaImageProcessor._preprocess_image\u001b[0;34m(self, image, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_map_pixels, data_format)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImage mean and std must be specified if do_normalize is True.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    458\u001b[0m \u001b[39m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m image \u001b[39m=\u001b[39m to_numpy_array(image)\n\u001b[1;32m    461\u001b[0m \u001b[39mif\u001b[39;00m do_resize:\n\u001b[1;32m    462\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresize(image\u001b[39m=\u001b[39mimage, size\u001b[39m=\u001b[39msize, resample\u001b[39m=\u001b[39mresample)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/image_utils.py:139\u001b[0m, in \u001b[0;36mto_numpy_array\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_numpy_array\u001b[39m(img) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_valid_image(img):\n\u001b[1;32m    140\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid image type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m is_vision_available() \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(img, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/image_utils.py:73\u001b[0m, in \u001b[0;36mis_valid_image\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_valid_image\u001b[39m(img):\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m---> 73\u001b[0m         (is_vision_available() \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(img, PIL\u001b[39m.\u001b[39mImage\u001b[39m.\u001b[39mImage))\n\u001b[1;32m     74\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(img, np\u001b[39m.\u001b[39mndarray)\n\u001b[1;32m     75\u001b[0m         \u001b[39mor\u001b[39;00m is_torch_tensor(img)\n\u001b[1;32m     76\u001b[0m         \u001b[39mor\u001b[39;00m is_tf_tensor(img)\n\u001b[1;32m     77\u001b[0m         \u001b[39mor\u001b[39;00m is_jax_tensor(img)\n\u001b[1;32m     78\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py:566\u001b[0m, in \u001b[0;36mis_vision_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39mif\u001b[39;00m _pil_available:\n\u001b[1;32m    565\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m         package_version \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mmetadata\u001b[39m.\u001b[39;49mversion(\u001b[39m\"\u001b[39;49m\u001b[39mPillow\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    567\u001b[0m     \u001b[39mexcept\u001b[39;00m importlib\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mPackageNotFoundError:\n\u001b[1;32m    568\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.9/importlib/metadata.py:551\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mversion\u001b[39m(distribution_name):\n\u001b[1;32m    545\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \n\u001b[1;32m    547\u001b[0m \u001b[39m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[39m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[39m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 551\u001b[0m     \u001b[39mreturn\u001b[39;00m distribution(distribution_name)\u001b[39m.\u001b[39;49mversion\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/importlib_metadata/__init__.py:472\u001b[0m, in \u001b[0;36mDistribution.version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    470\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mversion\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    471\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the 'Version' metadata for the distribution package.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m'\u001b[39;49m\u001b[39mVersion\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/importlib_metadata/_adapters.py:59\u001b[0m, in \u001b[0;36mMessage.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, item):\n\u001b[1;32m     55\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m    Warn users that a ``KeyError`` can be expected when a\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m    mising key is supplied. Ref python/importlib_metadata#371.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(item)\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         _warn()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics, amca_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.evaluation.metrics.checkpoint import WeightCheckpoint\n",
    "from avalanche.training.plugins import EvaluationPlugin, GEMPlugin, GDumbPlugin\n",
    "from avalanche.training.supervised import Naive, ER_ACE\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "from avalanche.training.plugins import ReplayPlugin\n",
    "\n",
    "\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "#Chnage these wandb parameters for every separate run\n",
    "wandb_logger = WandBLogger(\n",
    "        project_name=\"FLAVA frozen model with CUB200 Experience Replay CL\",\n",
    "        run_name=\"Flava 1 child layers frozen\",\n",
    "        log_artifacts=True,\n",
    "        path=None,\n",
    "        \n",
    "    )\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    amca_metrics(),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario_DIL.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, tb_logger, wandb_logger]\n",
    ")\n",
    "\n",
    "\n",
    "replay_plugin = ReplayPlugin(\n",
    "        mem_size=50)\n",
    "\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (ER)\n",
    "cl_strategy_DIL = Naive(\n",
    "    flava_spec_frozen, SGD(flava_spec_frozen.parameters(), lr=0.09, momentum=0.9),\n",
    "    CrossEntropyLoss(), train_mb_size=100, train_epochs=150, eval_mb_size=25, \n",
    "    evaluator=eval_plugin,plugins=[replay_plugin], device=device)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in scenario_DIL.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy_DIL.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy_DIL.eval(scenario_DIL.test_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "# image_processor = AutoImageProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "# model_custom = FlavaImageModel.from_pretrained(\"facebook/flava-full\").eval().to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# child_counter = 0\n",
    "# for child in model_custom.children():\n",
    "#     children_of_child_counter = 0\n",
    "#     print(\"The children no. \", child_counter, \" is \", child )\n",
    "#     if child_counter == 1:\n",
    "#         for sub_child in child.named_modules():\n",
    "#             print(\"The sub child no. \", children_of_child_counter, \" is \", sub_child )\n",
    "#             children_of_child_counter +=1\n",
    "#             if children_of_child_counter < 182:\n",
    "#                 for param in sub_child.parameters():\n",
    "#                     param.requires_grad = False\n",
    "#             else:\n",
    "#                 pass\n",
    "#     else:\n",
    "#         pass\n",
    "#     child_counter +=1\n",
    "\n",
    "\n",
    "# child_counter = 0\n",
    "# for child in model_custom.children():\n",
    "#     #print(\"The children no. \", child_counter, \" is \", child )\n",
    "#     child_counter += 1\n",
    "#     if child_counter < 2:\n",
    "#         for param in child.parameters():\n",
    "#             param.requires_grad = False\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
