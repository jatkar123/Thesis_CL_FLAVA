{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries\n",
    "from transformers import AutoProcessor, FlavaModel, FlavaImageModel, AutoImageProcessor, FlavaFeatureExtractor\n",
    "import numpy as np\n",
    "from torchmultimodal.models.flava.model import flava_model_for_classification\n",
    "from transformers import FlavaProcessor, FlavaForPreTraining\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available on this system.\n"
     ]
    }
   ],
   "source": [
    "#Checking for GPU and settingt he device as GPU if avaialble\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available on this system.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=device, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from avalanche.benchmarks.generators import nc_benchmark, ni_benchmark\n",
    "from avalanche.benchmarks.datasets import MNIST, CUB200\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "resize_transform = transforms.Resize((224, 224))\n",
    "grayscale_transform = transforms.Grayscale(num_output_channels=3)\n",
    "train_transform = Compose([\n",
    "    resize_transform,\n",
    "    grayscale_transform,\n",
    "    ToTensor()\n",
    "    \n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    resize_transform,\n",
    "    grayscale_transform,\n",
    "    ToTensor()\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "mnist_train = MNIST(\n",
    "    './data/mnist', train=True, download=True, transform=train_transform\n",
    ")\n",
    "\n",
    "mnist_test = MNIST(\n",
    "    './data/mnist', train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "# cub_train = CUB200(\n",
    "#     './data/cub200', train=True, download=True, transform=train_transform\n",
    "# )\n",
    "\n",
    "# cub_test = CUB200(\n",
    "#     './data/cub200', train=False, download=True,  transform=test_transform\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size: <built-in method size of Tensor object at 0x7f6622b30310>\n",
      "Image Shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#Checking for image shape and size\n",
    "# Get the first image from the dataset\n",
    "image, label = mnist_train[0]\n",
    "\n",
    "# Check the size and shape of the image\n",
    "size = image.size\n",
    "shape = image.shape\n",
    "\n",
    "print(f\"Image Size: {size}\")\n",
    "print(f\"Image Shape: {shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classsic class incremental creation from MNIST dataset\n",
    "scenario_CIL = nc_benchmark(\n",
    "    mnist_train, mnist_test, n_experiences=5, shuffle=True, seed=1234,\n",
    "    task_labels=True\n",
    ")\n",
    "\n",
    "\n",
    "#Classsic Domain incremental creation from MNIST dataset\n",
    "scenario_DIL = ni_benchmark(\n",
    "    mnist_train, mnist_test, n_experiences=3, shuffle=False, seed=42,\n",
    "    balance_experiences=True \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a FLAVA custom class which calls the pretrained image model (IM encoder only) and returns the features/embeddings instead of a flava basepooling type\n",
    "\n",
    "class flava_custom(torch.nn.Module):\n",
    "    def __init__(self, num_classes, device='cuda'):\n",
    "        super().__init__()\n",
    "        #self.image_processor = AutoImageProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "        #self.model_custom = FlavaImageModel.from_pretrained(\"facebook/flava-full\").to(\"cuda\")\n",
    "\n",
    "        self.image_processor = FlavaFeatureExtractor.from_pretrained(\"facebook/flava-full\") \n",
    "        self.model_custom = FlavaModel.from_pretrained(\"facebook/flava-full\", low_cpu_mem_usage=True).eval().cuda()\n",
    "\n",
    "        #self.image_processor = FlavaProcessor.from_pretrained(\"facebook/flava-full\", return_codebook_pixels=True)\n",
    "        #self.model_custom = FlavaForPreTraining.from_pretrained(\"facebook/flava-full\").to(\"cuda\")\n",
    "        \n",
    "        \n",
    "        #self.model_custom = FlavaModel.from_pretrained(\"facebook/flava-full\").eval().to(\"cuda\")\n",
    "        #self.image_processor = AutoProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, list):\n",
    "            x = list(x.cuda())\n",
    "         \n",
    "        \n",
    "        inputs = self.image_processor(images=x, return_tensors=\"pt\").to(\"cuda\")\n",
    "        #with torch.no_grad():\n",
    "        outputs = self.model_custom.get_image_features(**inputs)[:, 0, :] \n",
    "        \n",
    "        return outputs  #.last_hidden_state[:, 0, :]\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "        # score = outputs.last_hidden_state[:, 0, :] \n",
    "\n",
    "        # # Passing through a fully connected layer and the number of output classes\n",
    "        # fc_layer = nn.Linear(in_features=score.size(1), out_features=self.num_classes).to(self.device)\n",
    "\n",
    "        # # # Get the logits by passing image_features through the fc_layer\n",
    "        # logits = fc_layer(score).to(self.device)\n",
    "        \n",
    "        \n",
    "        # return logits  #last_hidden_state[:, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/transformers/models/flava/feature_extraction_flava.py:28: FutureWarning: The class FlavaFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use FlavaImageProcessor instead.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x7f65d0fac630>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.3674, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "image_tensor_with_batch = torch.unsqueeze(mnist_train[0][0], 0)\n",
    "custom_DIL = flava_custom(num_classes=scenario_CIL.n_classes)\n",
    "f = custom_DIL.forward(x=image_tensor_with_batch)\n",
    "print(f.size)\n",
    "input_ids = torch.argmax(f, dim=1).flatten(0) #important line to convert labels to match dimensions\n",
    "loss = torch.nn.functional.cross_entropy(f, input_ids)\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Domain incremental setting on SplitMNIST using EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 201/201 [07:27<00:00,  2.22s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.1775\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1713\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0108\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 446.9590\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0984\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|██████████| 201/201 [07:27<00:00,  2.23s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3835\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1345\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0097\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 447.2434\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1034\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|██████████| 201/201 [07:26<00:00,  2.22s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3199\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2551\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0106\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 445.9123\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1045\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  1\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 200/200 [07:43<00:00,  2.32s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3137\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2967\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0101\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 462.9446\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1017\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1111\n",
      "100%|██████████| 200/200 [07:43<00:00,  2.32s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3124\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3248\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0099\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 462.9542\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1017\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1414\n",
      "100%|██████████| 200/200 [07:42<00:00,  2.31s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3162\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2779\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0100\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 462.4764\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1014\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1616\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  2\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 200/200 [07:54<00:00,  2.37s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3201\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3372\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0108\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 474.6850\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1022\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0619\n",
      "100%|██████████| 200/200 [07:56<00:00,  2.38s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3253\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3238\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0101\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 475.9008\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0967\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0825\n",
      "100%|██████████| 200/200 [07:55<00:00,  2.38s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447198.0889\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3324\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3337\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0102\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 475.1717\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1009\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0515\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 400/400 [04:08<00:00,  1.61it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 311.0671\n",
      "\tDiskUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 2447198.0889\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 2.3562\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.1028\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,  980,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0, 1135,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0, 1032,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0, 1010,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,  982,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,  892,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,  958,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0, 1028,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,  974,    0,    0],\n",
      "        [   0,    0,    0,    0,    0,    0,    0, 1009,    0,    0]])\n",
      "\tDiskUsage_Stream/eval_phase/test_stream/Task000 = 2447198.0889\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 2.3562\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.0000\n",
      "\tTop1_AMCA_Stream/test_stream/Task000 = 0.1000\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1028\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics, amca_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, EWCPlugin\n",
    "from avalanche.training.supervised import Naive, EWC\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "\n",
    "\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "\n",
    "custom_DIL = flava_custom(num_classes=10)\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    amca_metrics(),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario_DIL.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger]#, tb_logger]\n",
    ")\n",
    "\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (EWC)\n",
    "cl_strategy_DIL = EWC(\n",
    "    custom_DIL, Adam(custom_DIL.parameters(), lr=0.1),\n",
    "    CrossEntropyLoss(), ewc_lambda=0.4, train_mb_size=100, train_epochs=3, eval_mb_size=25,\n",
    "    evaluator=eval_plugin, device=device) \n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in scenario_DIL.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy_DIL.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "print('Computing accuracy on the whole test set')\n",
    "# test also returns a dictionary which contains all the metric values\n",
    "results.append(cl_strategy_DIL.eval(scenario_DIL.test_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIL setting with EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [4, 5]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 113/113 [04:11<00:00,  2.23s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.2708\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.7144\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0181\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 251.3411\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4903\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5238\n",
      "100%|██████████| 113/113 [04:14<00:00,  2.25s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.7066\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6952\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0170\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 254.2616\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4984\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3968\n",
      "100%|██████████| 113/113 [04:13<00:00,  2.24s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6953\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6568\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0167\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 252.9401\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5063\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6667\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  1\n",
      "Current Classes:  [8, 2]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 119/119 [04:34<00:00,  2.31s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task001 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task001 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task001 = 1.2471\n",
      "\tLoss_MB/train_phase/train_stream/Task001 = 0.7453\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task001 = 0.0169\n",
      "\tTime_Epoch/train_phase/train_stream/Task001 = 274.6049\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task001 = 0.4785\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task001 = 0.5556\n",
      "100%|██████████| 119/119 [04:33<00:00,  2.30s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task001 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task001 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task001 = 0.7049\n",
      "\tLoss_MB/train_phase/train_stream/Task001 = 0.7108\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task001 = 0.0166\n",
      "\tTime_Epoch/train_phase/train_stream/Task001 = 271.3086\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task001 = 0.4905\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task001 = 0.4444\n",
      "100%|██████████| 119/119 [04:34<00:00,  2.31s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task001 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task001 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task001 = 0.6995\n",
      "\tLoss_MB/train_phase/train_stream/Task001 = 0.6934\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task001 = 0.0185\n",
      "\tTime_Epoch/train_phase/train_stream/Task001 = 274.2466\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task001 = 0.5054\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task001 = 0.5556\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  2\n",
      "Current Classes:  [9, 6]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 119/119 [04:39<00:00,  2.35s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task002 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task002 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task002 = 1.3766\n",
      "\tLoss_MB/train_phase/train_stream/Task002 = 0.6998\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task002 = 0.0179\n",
      "\tTime_Epoch/train_phase/train_stream/Task002 = 279.4559\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task002 = 0.4590\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task002 = 0.5522\n",
      "100%|██████████| 119/119 [04:39<00:00,  2.35s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task002 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task002 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task002 = 0.7041\n",
      "\tLoss_MB/train_phase/train_stream/Task002 = 0.7165\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task002 = 0.0170\n",
      "\tTime_Epoch/train_phase/train_stream/Task002 = 278.9222\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task002 = 0.4989\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task002 = 0.4627\n",
      "100%|██████████| 119/119 [04:39<00:00,  2.35s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task002 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task002 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task002 = 0.7040\n",
      "\tLoss_MB/train_phase/train_stream/Task002 = 0.7290\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task002 = 0.0169\n",
      "\tTime_Epoch/train_phase/train_stream/Task002 = 278.8021\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task002 = 0.4951\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task002 = 0.3731\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  3\n",
      "Current Classes:  [1, 7]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 131/131 [05:16<00:00,  2.41s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task003 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task003 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task003 = 1.2426\n",
      "\tLoss_MB/train_phase/train_stream/Task003 = 0.7032\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task003 = 0.0169\n",
      "\tTime_Epoch/train_phase/train_stream/Task003 = 316.0302\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task003 = 0.4609\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task003 = 0.5714\n",
      "100%|██████████| 131/131 [05:14<00:00,  2.40s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task003 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task003 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task003 = 0.7109\n",
      "\tLoss_MB/train_phase/train_stream/Task003 = 0.6170\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task003 = 0.0161\n",
      "\tTime_Epoch/train_phase/train_stream/Task003 = 314.6088\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task003 = 0.5017\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task003 = 0.7143\n",
      "100%|██████████| 131/131 [05:15<00:00,  2.41s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task003 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task003 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task003 = 0.7105\n",
      "\tLoss_MB/train_phase/train_stream/Task003 = 0.6263\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task003 = 0.0161\n",
      "\tTime_Epoch/train_phase/train_stream/Task003 = 315.5705\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task003 = 0.5020\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task003 = 0.7143\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  4\n",
      "Current Classes:  [0, 3]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 121/121 [04:59<00:00,  2.48s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task004 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task004 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task004 = 1.3714\n",
      "\tLoss_MB/train_phase/train_stream/Task004 = 0.7160\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task004 = 0.0185\n",
      "\tTime_Epoch/train_phase/train_stream/Task004 = 299.2936\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task004 = 0.4642\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task004 = 0.4815\n",
      "100%|██████████| 121/121 [05:01<00:00,  2.49s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task004 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task004 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task004 = 0.7261\n",
      "\tLoss_MB/train_phase/train_stream/Task004 = 0.7518\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task004 = 0.0179\n",
      "\tTime_Epoch/train_phase/train_stream/Task004 = 300.7945\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task004 = 0.4957\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task004 = 0.4074\n",
      "100%|██████████| 121/121 [05:00<00:00,  2.48s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task004 = 2730897.1426\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task004 = 2730897.1426\n",
      "\tLoss_Epoch/train_phase/train_stream/Task004 = 0.7134\n",
      "\tLoss_MB/train_phase/train_stream/Task004 = 0.7098\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task004 = 0.0180\n",
      "\tTime_Epoch/train_phase/train_stream/Task004 = 300.0254\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task004 = 0.4932\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task004 = 0.4815\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 75/75 [00:48<00:00,  1.55it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 299.5038\n",
      "\tDiskUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 2730897.1426\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 15.3460\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 1) from test stream --\n",
      "100%|██████████| 81/81 [00:50<00:00,  1.62it/s]\n",
      "> Eval on experience 1 (Task 1) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task001/Exp001 = 314.1399\n",
      "\tDiskUsage_Exp/eval_phase/test_stream/Task001/Exp001 = 2730897.1426\n",
      "\tLoss_Exp/eval_phase/test_stream/Task001/Exp001 = 28.5285\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task001/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 2) from test stream --\n",
      "100%|██████████| 79/79 [00:48<00:00,  1.62it/s]\n",
      "> Eval on experience 2 (Task 2) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task002/Exp002 = 307.5173\n",
      "\tDiskUsage_Exp/eval_phase/test_stream/Task002/Exp002 = 2730897.1426\n",
      "\tLoss_Exp/eval_phase/test_stream/Task002/Exp002 = 17.6260\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task002/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 3) from test stream --\n",
      "100%|██████████| 87/87 [00:53<00:00,  1.62it/s]\n",
      "> Eval on experience 3 (Task 3) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task003/Exp003 = 305.3196\n",
      "\tDiskUsage_Exp/eval_phase/test_stream/Task003/Exp003 = 2730897.1426\n",
      "\tLoss_Exp/eval_phase/test_stream/Task003/Exp003 = 21.1377\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task003/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 4) from test stream --\n",
      "100%|██████████| 80/80 [00:49<00:00,  1.62it/s]\n",
      "> Eval on experience 4 (Task 4) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task004/Exp004 = 310.4399\n",
      "\tDiskUsage_Exp/eval_phase/test_stream/Task004/Exp004 = 2730897.1426\n",
      "\tLoss_Exp/eval_phase/test_stream/Task004/Exp004 = 0.6936\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task004/Exp004 = 0.4925\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[ 980,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1135,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1032,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1010,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 982,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 892,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 958,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1028,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 974,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1009,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "\tDiskUsage_Stream/eval_phase/test_stream/Task004 = 2730897.1426\n",
      "\tLoss_Stream/eval_phase/test_stream/Task004 = 16.7758\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.0000\n",
      "\tTop1_AMCA_Stream/test_stream/Task000 = 0.0000\n",
      "\tTop1_AMCA_Stream/test_stream/Task001 = 0.0000\n",
      "\tTop1_AMCA_Stream/test_stream/Task002 = 0.0000\n",
      "\tTop1_AMCA_Stream/test_stream/Task003 = 0.0000\n",
      "\tTop1_AMCA_Stream/test_stream/Task004 = 0.5000\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task004 = 0.0980\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics, amca_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, EWCPlugin\n",
    "from avalanche.training.supervised import Naive, EWC\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "\n",
    "\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "\n",
    "custom_TIL = flava_custom(num_classes=10)\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    amca_metrics(),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario_CIL.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger]#, tb_logger]\n",
    ")\n",
    "\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (EWC)\n",
    "cl_strategy_TIL = EWC(\n",
    "    custom_TIL, SGD(custom_TIL.parameters(), lr=0.01, momentum=0.9),\n",
    "    CrossEntropyLoss(), ewc_lambda=0.4, train_mb_size=100, train_epochs=3, eval_mb_size=25,\n",
    "    evaluator=eval_plugin, device=device) \n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in scenario_CIL.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy_TIL.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy_TIL.eval(scenario_CIL.test_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EWC DIL on SMNIST using a MLP on top of custom_flava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPClassificationHead(flava_custom):\n",
    "    def __init__(self, num_classes, hidden_dim, device='cuda'):\n",
    "        super().__init__(num_classes=num_classes, device=device)\n",
    "        \n",
    "        # Add an MLP classification head layers\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(self.model_custom.config.hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Call the forward method of the base class\n",
    "        features = super().forward(x)\n",
    "        \n",
    "        # Pass the features through the classification head\n",
    "        logits = self.classification_head(features)\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "hidden_dim = 256\n",
    "\n",
    "#model = MLPClassificationHead(num_classes=num_classes, hidden_dim=hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/transformers/models/flava/feature_extraction_flava.py:28: FutureWarning: The class FlavaFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use FlavaImageProcessor instead.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 201/201 [07:30<00:00,  2.24s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2887\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2880\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0103\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 449.8511\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1701\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|██████████| 201/201 [07:30<00:00,  2.24s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2729\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2789\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0097\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 450.6325\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1657\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|██████████| 201/201 [07:32<00:00,  2.25s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3016\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2909\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0097\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 452.2970\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1124\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  1\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 200/200 [07:45<00:00,  2.33s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2996\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3014\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0098\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 465.2064\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1149\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1111\n",
      "100%|██████████| 200/200 [07:50<00:00,  2.35s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3015\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3030\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0098\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 470.5132\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1118\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1010\n",
      "100%|██████████| 200/200 [07:48<00:00,  2.34s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3014\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3117\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0102\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 467.7943\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1124\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0404\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  2\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 200/200 [07:57<00:00,  2.39s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3014\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3012\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0108\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 477.4036\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1124\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0928\n",
      "100%|██████████| 200/200 [07:55<00:00,  2.38s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3014\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3048\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0101\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 475.1921\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1124\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1134\n",
      "100%|██████████| 200/200 [07:54<00:00,  2.37s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730915.9590\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3014\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3047\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0103\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 474.0825\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1124\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0722\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 400/400 [04:10<00:00,  1.60it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 307.2476\n",
      "\tDiskUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 2730915.9590\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 2.3010\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.1135\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[   0,  980,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0, 1135,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0, 1032,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0, 1010,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,  982,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,  892,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,  958,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0, 1028,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,  974,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0, 1009,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "\tDiskUsage_Stream/eval_phase/test_stream/Task000 = 2730915.9590\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 2.3010\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.0000\n",
      "\tTop1_AMCA_Stream/test_stream/Task000 = 0.1000\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1135\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics, amca_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, EWCPlugin\n",
    "from avalanche.training.supervised import Naive, EWC\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "\n",
    "\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "\n",
    "custom_DIL_MLP =  MLPClassificationHead(num_classes=num_classes, hidden_dim=hidden_dim) #flava_custom(num_classes=10)\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    amca_metrics(),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario_DIL.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger]#, tb_logger]\n",
    ")\n",
    "\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (EWC)\n",
    "cl_strategy_DIL = EWC(\n",
    "    custom_DIL_MLP, SGD(custom_DIL_MLP.parameters(), lr=0.01),\n",
    "    CrossEntropyLoss(), ewc_lambda=0.4, train_mb_size=100, train_epochs=3, eval_mb_size=25,\n",
    "    evaluator=eval_plugin, device=device) \n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in scenario_DIL.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy_DIL.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "print('Computing accuracy on the whole test set')\n",
    "# test also returns a dictionary which contains all the metric values\n",
    "results.append(cl_strategy_DIL.eval(scenario_DIL.test_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EWC on ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name vit_base_patch16_224_in21k to current vit_base_patch16_224.augreg_in21k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "model_vit = timm.create_model('vit_base_patch16_224_in21k', num_classes=10, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 121/121 [02:49<00:00,  1.40s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 9.3004\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2735\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0187\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 169.4870\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1025\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2000\n",
      "100%|██████████| 121/121 [03:03<00:00,  1.52s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4527\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4441\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0098\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 183.3850\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1122\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.4000\n",
      "100%|██████████| 121/121 [03:03<00:00,  1.52s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3638\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1607\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0096\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 183.3057\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1150\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.4000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  1\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 121/121 [02:51<00:00,  1.41s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3155\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3810\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0111\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 170.8424\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1446\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|██████████| 121/121 [02:49<00:00,  1.40s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2823\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3650\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0092\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 169.4520\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1699\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|██████████| 121/121 [02:50<00:00,  1.41s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2078\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7061\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0093\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 169.9394\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1886\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  2\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 120/120 [02:52<00:00,  1.44s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0771\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0377\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0100\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 172.4009\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2226\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2100\n",
      "100%|██████████| 120/120 [02:51<00:00,  1.43s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0093\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9088\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0096\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 171.1855\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2447\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|██████████| 120/120 [02:53<00:00,  1.44s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8979\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7050\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0095\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 172.7506\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2717\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.4000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  3\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 120/120 [02:54<00:00,  1.46s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7167\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.5447\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0141\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 174.4514\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3159\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3093\n",
      "100%|██████████| 120/120 [02:54<00:00,  1.45s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6878\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7096\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0096\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 173.9550\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3335\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3299\n",
      "100%|██████████| 120/120 [02:52<00:00,  1.44s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7032\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.4246\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0096\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 172.7028\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3547\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5052\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  4\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 120/120 [02:56<00:00,  1.47s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.1979\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.0465\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0141\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 176.4004\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5706\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6562\n",
      "100%|██████████| 120/120 [03:11<00:00,  1.59s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.0437\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.7442\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0101\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 189.1240\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6430\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7396\n",
      "100%|██████████| 120/120 [03:09<00:00,  1.58s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2447257.3984\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.7568\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.5365\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0100\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 188.9461\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.7604\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8125\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 400/400 [02:31<00:00,  2.63it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 343.6598\n",
      "\tDiskUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 2447257.3984\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.6699\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.7924\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[ 843,    0,    6,   33,    0,    9,    3,    2,   84,    0],\n",
      "        [   1, 1028,    3,    5,    5,    0,    3,   30,   47,   13],\n",
      "        [  42,    1,  629,  120,    8,   11,   32,    8,  157,   24],\n",
      "        [   8,    0,    4,  897,    0,   46,    0,    7,   25,   23],\n",
      "        [   6,    4,    4,    0,  705,    0,   15,    5,   34,  209],\n",
      "        [   3,    0,    3,  130,    1,  721,    3,    0,   23,    8],\n",
      "        [  24,    2,   47,    2,   20,   48,  721,    0,   94,    0],\n",
      "        [   4,    1,    9,   37,    2,    2,    0,  828,   16,  129],\n",
      "        [   3,    0,    7,   83,    1,   93,    5,    7,  729,   46],\n",
      "        [  10,    5,    2,   36,   27,   14,    0,   31,   61,  823]])\n",
      "\tDiskUsage_Stream/eval_phase/test_stream/Task000 = 2447257.3984\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 0.6699\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.0000\n",
      "\tTop1_AMCA_Stream/test_stream/Task000 = 0.7912\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.7924\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics, amca_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive, EWC\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "\n",
    "\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    amca_metrics(),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario_DIL.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger]#, tb_logger]\n",
    ")\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (EWC)\n",
    "cl_strategy_DIL = EWC(\n",
    "    model_vit, SGD(model_vit.parameters(), lr=0.01, momentum=0.9),\n",
    "    CrossEntropyLoss(), ewc_lambda=0.4, train_mb_size=100, train_epochs=3, eval_mb_size=25,\n",
    "    evaluator=eval_plugin, device=device)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in scenario_DIL.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy_DIL.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "print('Computing accuracy on the whole test set')\n",
    "# test also returns a dictionary which contains all the metric values\n",
    "results.append(cl_strategy_DIL.eval(scenario_DIL.test_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EWC on CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class clip_custom(torch.nn.Module):\n",
    "    def __init__(self, num_classes, device='cuda'):\n",
    "        super().__init__()\n",
    "        #self.image_processor = AutoImageProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "        #self.model_custom = FlavaImageModel.from_pretrained(\"facebook/flava-full\").to(\"cuda\")\n",
    "\n",
    "        self.image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.model_custom = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, list):\n",
    "            x = list(x.cuda())\n",
    "         \n",
    "        \n",
    "        inputs = self.image_processor(images=x, return_tensors=\"pt\").to(\"cuda\")\n",
    "        #with torch.no_grad():\n",
    "        outputs = self.model_custom(**inputs)\n",
    "        \n",
    "        return outputs.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 201/201 [05:08<00:00,  1.53s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = nan\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = nan\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0063\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 308.1033\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0953\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|██████████| 201/201 [05:09<00:00,  1.54s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = nan\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = nan\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0070\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 309.3742\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0987\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|██████████| 201/201 [05:13<00:00,  1.56s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = nan\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = nan\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0062\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 312.8861\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0987\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  1\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 200/200 [05:19<00:00,  1.60s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = nan\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = nan\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0064\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 318.8618\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0987\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0909\n",
      "100%|██████████| 200/200 [05:19<00:00,  1.60s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = nan\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = nan\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0065\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 318.9145\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0987\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1010\n",
      "100%|██████████| 200/200 [05:15<00:00,  1.58s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = nan\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = nan\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0064\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 314.8556\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0987\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0808\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  2\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 200/200 [05:24<00:00,  1.62s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = nan\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = nan\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0068\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 324.1044\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0987\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1237\n",
      "100%|██████████| 200/200 [05:17<00:00,  1.59s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = nan\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = nan\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 317.0624\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0987\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1546\n",
      "100%|██████████| 200/200 [05:21<00:00,  1.61s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2729632.3066\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = nan\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = nan\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 320.8244\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0987\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0515\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Computing accuracy on the whole test set\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 400/400 [03:50<00:00,  1.74it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tCPUUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 320.0284\n",
      "\tDiskUsage_Exp/eval_phase/test_stream/Task000/Exp000 = 2729632.3066\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = nan\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0980\n",
      "-- >> End of eval phase << --\n",
      "\tConfusionMatrix_Stream/eval_phase/test_stream = \n",
      "tensor([[ 980,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1135,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1032,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1010,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 982,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 892,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 958,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1028,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 974,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1009,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "\tDiskUsage_Stream/eval_phase/test_stream/Task000 = 2729632.3066\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = nan\n",
      "\tStreamForgetting/eval_phase/test_stream = 0.0000\n",
      "\tTop1_AMCA_Stream/test_stream/Task000 = 0.1000\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0980\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics, amca_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, EWCPlugin\n",
    "from avalanche.training.supervised import Naive, EWC\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "\n",
    "\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "\n",
    "custom_DIL_clip = clip_custom(num_classes=10)\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    amca_metrics(),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario_DIL.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger]#, tb_logger]\n",
    ")\n",
    "\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (EWC)\n",
    "cl_strategy_DIL = EWC(\n",
    "    custom_DIL_clip, SGD(custom_DIL_clip.parameters(), lr=0.1),\n",
    "    CrossEntropyLoss(), ewc_lambda=0.4, train_mb_size=100, train_epochs=3, eval_mb_size=25,\n",
    "    evaluator=eval_plugin, device=device) \n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in scenario_DIL.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy_DIL.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "print('Computing accuracy on the whole test set')\n",
    "# test also returns a dictionary which contains all the metric values\n",
    "results.append(cl_strategy_DIL.eval(scenario_DIL.test_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CL with Vision transformer but in the same custom class format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification, ViTModel\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vit_custom_logit(torch.nn.Module):\n",
    "    def __init__(self, num_classes, device='cuda'):\n",
    "        super().__init__()\n",
    "        #self.image_processor = AutoImageProcessor.from_pretrained(\"facebook/flava-full\")\n",
    "        #self.model_custom = FlavaImageModel.from_pretrained(\"facebook/flava-full\").to(\"cuda\")\n",
    "\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        self.model_custom = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(\"cuda\")\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, list):\n",
    "            x = list(x.cuda())\n",
    "         \n",
    "        \n",
    "        inputs = self.image_processor(images=x, return_tensors=\"pt\").to(\"cuda\")\n",
    "        #with torch.no_grad():\n",
    "        outputs = self.model_custom(**inputs).logits\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 201/201 [07:16<00:00,  2.17s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6704\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1703\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0160\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 436.4763\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1006\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|██████████| 201/201 [07:10<00:00,  2.14s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3615\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4707\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0090\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 430.3028\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1013\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|██████████| 201/201 [07:13<00:00,  2.16s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3582\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4014\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0091\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 433.0005\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1018\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  1\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 200/200 [07:24<00:00,  2.22s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3316\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2866\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0096\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 444.3379\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1008\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1616\n",
      "100%|██████████| 200/200 [07:21<00:00,  2.21s/it]\n",
      "Epoch 1 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3254\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2928\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0095\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 440.8922\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1055\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1010\n",
      "100%|██████████| 200/200 [07:14<00:00,  2.17s/it]\n",
      "Epoch 2 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3252\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3411\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0093\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 434.6972\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1035\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1212\n",
      "-- >> End of training phase << --\n",
      "Training completed\n",
      "Start of experience:  2\n",
      "Current Classes:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 200/200 [07:24<00:00,  2.22s/it]\n",
      "Epoch 0 ended.\n",
      "\tDiskUsage_Epoch/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tDiskUsage_MB/train_phase/train_stream/Task000 = 2730870.9688\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3269\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3022\n",
      "\tRunningTime_Epoch/train_phase/train_stream/Task000 = 0.0095\n",
      "\tTime_Epoch/train_phase/train_stream/Task000 = 443.9361\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0992\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1340\n",
      " 10%|█         | 20/200 [00:43<06:25,  2.14s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent Classes: \u001b[39m\u001b[39m\"\u001b[39m, experience\u001b[39m.\u001b[39mclasses_in_this_experience)\n\u001b[1;32m     58\u001b[0m     \u001b[39m# train returns a dictionary which contains all the metric values\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     res \u001b[39m=\u001b[39m cl_strategy_DIL\u001b[39m.\u001b[39;49mtrain(experience)\n\u001b[1;32m     60\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining completed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mComputing accuracy on the whole test set\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/training/templates/base_sgd.py:168\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\n\u001b[1;32m    161\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    162\u001b[0m     experiences: Union[TDatasetExperience, Iterable[TDatasetExperience]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    167\u001b[0m ):\n\u001b[0;32m--> 168\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtrain(experiences, eval_streams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_last_metrics()\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/training/templates/base.py:144\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperience \u001b[39min\u001b[39;00m experiences_list:\n\u001b[1;32m    143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_before_training_exp(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 144\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_exp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperience, eval_streams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training_exp(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/training/templates/base_sgd.py:296\u001b[0m, in \u001b[0;36mBaseSGDTemplate._train_exp\u001b[0;34m(self, experience, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_epoch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training_epoch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/training/templates/update_type/sgd_update.py:38\u001b[0m, in \u001b[0;36mSGDUpdate.training_epoch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_step()\n\u001b[1;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_update(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_after_training_iteration(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/training/templates/base_sgd.py:487\u001b[0m, in \u001b[0;36mBaseSGDTemplate._after_training_iteration\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_after_training_iteration\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 487\u001b[0m     trigger_plugins(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mafter_training_iteration\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/training/utils.py:69\u001b[0m, in \u001b[0;36mtrigger_plugins\u001b[0;34m(strategy, event, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m strategy\u001b[39m.\u001b[39mplugins:\n\u001b[1;32m     68\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(p, event):\n\u001b[0;32m---> 69\u001b[0m         \u001b[39mgetattr\u001b[39;49m(p, event)(strategy, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/training/plugins/evaluation.py:222\u001b[0m, in \u001b[0;36mEvaluationPlugin.__getattribute__.<locals>.fun\u001b[0;34m(strat, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfun\u001b[39m(strat, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 222\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_metrics_and_loggers(strat, item)\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/training/plugins/evaluation.py:160\u001b[0m, in \u001b[0;36mEvaluationPlugin._update_metrics_and_loggers\u001b[0;34m(self, strategy, callback)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics:\n\u001b[1;32m    159\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(metric, callback):\n\u001b[0;32m--> 160\u001b[0m         metric_result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(metric, callback)(strategy)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(metric_result, Sequence):\n\u001b[1;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m mval \u001b[39min\u001b[39;00m metric_result:\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/evaluation/metric_definitions.py:303\u001b[0m, in \u001b[0;36mGenericPluginMetric.after_training_iteration\u001b[0;34m(self, strategy)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mafter_training_iteration(strategy)\n\u001b[1;32m    302\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(strategy)\n\u001b[1;32m    304\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_emit_at \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miteration\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    305\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_package_result(strategy)\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/evaluation/metrics/disk_usage.py:115\u001b[0m, in \u001b[0;36mDiskPluginMetric.update\u001b[0;34m(self, strategy)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, strategy):\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_metric\u001b[39m.\u001b[39;49mupdate()\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/evaluation/metrics/disk_usage.py:60\u001b[0m, in \u001b[0;36mDiskUsage.update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m dirs_size \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m directory \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_paths_to_monitor:\n\u001b[0;32m---> 60\u001b[0m     dirs_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m DiskUsage\u001b[39m.\u001b[39;49mget_dir_size(directory)\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_usage \u001b[39m=\u001b[39m dirs_size\n",
      "File \u001b[0;32m~/bentoml/Thesis/Custom_CL/avalanche_custom/avalanche/evaluation/metrics/disk_usage.py:94\u001b[0m, in \u001b[0;36mDiskUsage.get_dir_size\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39mObtains the size of the given directory, in KiB.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m    of the directory as the sum of all its elements.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m total_size \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 94\u001b[0m \u001b[39mfor\u001b[39;00m dirpath, dirnames, filenames \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mwalk(path):\n\u001b[1;32m     95\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m filenames:\n\u001b[1;32m     96\u001b[0m         fp \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dirpath, f)\n",
      "File \u001b[0;32m/usr/lib/python3.9/os.py:418\u001b[0m, in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[39m# Issue #23605: os.path.islink() is used instead of caching\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[39m# entry.is_symlink() result during the loop on os.scandir() because\u001b[39;00m\n\u001b[1;32m    415\u001b[0m         \u001b[39m# the caller can replace the directory entry during the \"yield\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m         \u001b[39m# above.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m         \u001b[39mif\u001b[39;00m followlinks \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m islink(new_path):\n\u001b[0;32m--> 418\u001b[0m             \u001b[39myield from\u001b[39;00m _walk(new_path, topdown, onerror, followlinks)\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[39m# Recurse into sub-directories\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[39mfor\u001b[39;00m new_path \u001b[39min\u001b[39;00m walk_dirs:\n",
      "File \u001b[0;32m/usr/lib/python3.9/os.py:418\u001b[0m, in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[39m# Issue #23605: os.path.islink() is used instead of caching\u001b[39;00m\n\u001b[1;32m    414\u001b[0m         \u001b[39m# entry.is_symlink() result during the loop on os.scandir() because\u001b[39;00m\n\u001b[1;32m    415\u001b[0m         \u001b[39m# the caller can replace the directory entry during the \"yield\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m         \u001b[39m# above.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m         \u001b[39mif\u001b[39;00m followlinks \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m islink(new_path):\n\u001b[0;32m--> 418\u001b[0m             \u001b[39myield from\u001b[39;00m _walk(new_path, topdown, onerror, followlinks)\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m     \u001b[39m# Recurse into sub-directories\u001b[39;00m\n\u001b[1;32m    421\u001b[0m     \u001b[39mfor\u001b[39;00m new_path \u001b[39min\u001b[39;00m walk_dirs:\n",
      "File \u001b[0;32m/usr/lib/python3.9/os.py:357\u001b[0m, in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39m# We may not have read permission for top, in which case we can't\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[39m# get a list of the files the directory contains.  os.walk\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[39m# always suppressed the exception then, rather than blow up for a\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39m# minor reason when (say) a thousand readable directories are still\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39m# left to visit.  That logic is copied here.\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m     \u001b[39m# Note that scandir is global in this module due\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[39m# to earlier import-*.\u001b[39;00m\n\u001b[0;32m--> 357\u001b[0m     scandir_it \u001b[39m=\u001b[39m scandir(top)\n\u001b[1;32m    358\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    359\u001b[0m     \u001b[39mif\u001b[39;00m onerror \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics, amca_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, EWCPlugin\n",
    "from avalanche.training.supervised import Naive, EWC\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "\n",
    "\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "\n",
    "custom_DIL_vit = vit_custom_logit(num_classes=10)\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    amca_metrics(),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario_DIL.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger]#, tb_logger]\n",
    ")\n",
    "\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (EWC)\n",
    "cl_strategy_DIL = EWC(\n",
    "    custom_DIL_vit, SGD(custom_DIL_vit.parameters(), lr=0.01),\n",
    "    CrossEntropyLoss(), ewc_lambda=0.4, train_mb_size=100, train_epochs=3, eval_mb_size=25,\n",
    "    evaluator=eval_plugin, device=device) \n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in scenario_DIL.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy_DIL.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy_DIL.eval(scenario_DIL.test_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded!\n"
     ]
    }
   ],
   "source": [
    "from continualai.colab.scripts import mnist\n",
    "mnist.init()\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train dim and type:  (60000, 1, 28, 28) float32\n",
      "t_train dim and type:  (60000,) uint8\n",
      "x_test dim and type:  (10000, 1, 28, 28) float32\n",
      "t_test dim and type:  (10000,) uint8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_train, t_train, x_test, t_test = mnist.load()\n",
    "\n",
    "print(\"x_train dim and type: \", x_train.shape, x_train.dtype)\n",
    "print(\"t_train dim and type: \", t_train.shape, t_train.dtype)\n",
    "print(\"x_test dim and type: \", x_test.shape, x_test.dtype)\n",
    "print(\"t_test dim and type: \", t_test.shape, t_test.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, dataset, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for img, label in tqdm(dataset):\n",
    "      \n",
    "      #print(i)\n",
    "      image_tensor_with_batch = torch.unsqueeze(img, 0)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      output = model.forward(image_tensor_with_batch)\n",
    "\n",
    "      input_ids = torch.argmax(output, dim=1).flatten(0)\n",
    "      loss = F.cross_entropy(output, input_ids)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      #print(loss.item())\n",
    "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "def test(model, device, dataset):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for img, label in (dataset):\n",
    "      \n",
    "      with torch.no_grad():\n",
    "        #x, y = torch.from_numpy(x_test[start:end]), torch.from_numpy(t_test[start:end]).long()\n",
    "        #x, y = x.to(device), y.to(device)\n",
    "        image_tensor_with_batch = torch.unsqueeze(img, 0)\n",
    "        output = model.forward(image_tensor_with_batch)\n",
    "        \n",
    "        # 'label' is an integer\n",
    "        label_array = np.array([label])\n",
    "        y = torch.from_numpy(label_array).to(\"cuda\")\n",
    "\n",
    "        test_loss += F.cross_entropy(output, y).item() # sum up batch loss\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(dataset),\n",
    "        100. * correct / len(dataset)))\n",
    "    return 100. * correct / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_rgb = np.concatenate([x_train, x_train, x_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_rgb  = np.concatenate([x_test, x_test, x_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_new(model, device, x_train, t_train, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for start in range(0, len(t_train)-1, 100):\n",
    "      end = start + 100\n",
    "      x, y = torch.from_numpy(x_train[start:end]), torch.from_numpy(t_train[start:end]).long()\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      output = model(x)\n",
    "      loss = F.cross_entropy(output, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      #print(loss.item())\n",
    "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "\n",
    "def test_new(model, device, x_test, t_test):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for start in range(0, len(t_test)-1, 256):\n",
    "      end = start + 256\n",
    "      with torch.no_grad():\n",
    "        x, y = torch.from_numpy(x_test[start:end]), torch.from_numpy(t_test[start:end]).long()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        test_loss += F.cross_entropy(output, y).item() # sum up batch loss\n",
    "        pred = output.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(t_test)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(t_test),\n",
    "        100. * correct / len(t_test)))\n",
    "    return 100. * correct / len(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/jraghu/.local/lib/python3.9/site-packages/transformers/models/flava/feature_extraction_flava.py:28: FutureWarning: The class FlavaFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use FlavaImageProcessor instead.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `FlavaTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`multimodal_config_dict` is provided which will be used to initialize `FlavaMultimodalConfig`. The value `multimodal_config[\"id2label\"]` will be overriden.\n",
      "`image_codebook_config_dict` is provided which will be used to initialize `FlavaImageCodebookConfig`. The value `image_codebook_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "trial_flava = flava_custom(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.SGD(trial_flava.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \tLoss: 2.312119\n",
      "Train Epoch: 2 \tLoss: 2.310108\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "  train_new(trial_flava, device, x_train_rgb, t_train, optimizer, epoch)\n",
    "#test_new(trial_flava, device, x_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0092, Accuracy: 958/10000 (10%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.58"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new(trial_flava, device, x_test_rgb, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from avalanche.benchmarks.datasets import MNIST\n",
    "# from torchvision.transforms import Compose, ToTensor, Lambda, RandomOrder\n",
    "\n",
    "# resize_transform = transforms.Resize((224, 224))\n",
    "# grayscale_transform = transforms.Grayscale(num_output_channels=3)\n",
    "# RandomHorizontalFlip_transform = transforms.RandomHorizontalFlip(p=0.5)\n",
    "# RandomVerticalFlip_transform = transforms.RandomVerticalFlip(p=0.5)\n",
    "# RandomRotation_transform = transforms.RandomRotation(degrees=90)\n",
    "\n",
    "# randomtransform = transforms.RandomOrder([RandomHorizontalFlip_transform, RandomVerticalFlip_transform, RandomRotation_transform])\n",
    "# train_transform = Compose([\n",
    "#     resize_transform,\n",
    "#     grayscale_transform,\n",
    "#     randomtransform,\n",
    "#     ToTensor()\n",
    "    \n",
    "# ])\n",
    "\n",
    "# # Apply the custom transform to the dataset\n",
    "# permuted_mnist_train = MNIST(root='./data/mnist', train=True, download=True,  transform=train_transform)\n",
    "# permuted_mnist_test = MNIST(root='./data/mnist', train=False, download=True,  transform=train_transform)\n",
    "def permute_mnist(mnist, seed):\n",
    "    \"\"\" Given the training set, permute pixels of each img the same way. \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    print(\"starting permutation...\")\n",
    "    h = w = 28\n",
    "    perm_inds = list(range(h*w))\n",
    "    np.random.shuffle(perm_inds)\n",
    "    # print(perm_inds)\n",
    "    perm_mnist = []\n",
    "    for set in mnist:\n",
    "        num_img = set.shape[0]\n",
    "        flat_set = set.reshape(num_img, w * h)\n",
    "        perm_mnist.append(flat_set[:, perm_inds].reshape(num_img, 1, w, h))\n",
    "    print(\"done.\")\n",
    "    return perm_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting permutation...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "x_train2, x_test2 = permute_mnist([x_train, x_test], 0)\n",
    "x_test2_rgb  = np.concatenate([x_test2, x_test2, x_test2], axis=1)\n",
    "x_train2_rgb = np.concatenate([x_train2, x_train2, x_train2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on the second task:\n",
      "Test set: Average loss: 0.0092, Accuracy: 958/10000 (10%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.58"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing on the second task:\")\n",
    "test_new(trial_flava, device, x_test2_rgb, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1\n",
    "task_1 = [(x_train_rgb, t_train), (x_test_rgb, t_test)]\n",
    "\n",
    "# task 2\n",
    "task_2 = [(x_train2_rgb, t_train), (x_test2_rgb, t_test)]\n",
    "\n",
    "fisher_dict = {}\n",
    "optpar_dict = {}\n",
    "ewc_lambda = 0.4\n",
    "\n",
    "\n",
    "# task list\n",
    "tasks = [task_1, task_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_task_update(task_id, x_mem, t_mem):\n",
    "\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  # accumulating gradients\n",
    "  for start in range(0, len(t_mem)-1, 100):\n",
    "      end = start + 100\n",
    "      x, y = torch.from_numpy(x_mem[start:end]), torch.from_numpy(t_mem[start:end]).long()\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      output = model(x)\n",
    "      loss = F.cross_entropy(output, y)\n",
    "      loss.backward()\n",
    "\n",
    "  fisher_dict[task_id] = {}\n",
    "  optpar_dict[task_id] = {}\n",
    "\n",
    "  # gradients accumulated can be used to calculate fisher\n",
    "  for name, param in model.named_parameters():\n",
    "    \n",
    "    optpar_dict[task_id][name] = param.data.clone()\n",
    "    fisher_dict[task_id][name] = param.grad.data.clone().pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ewc(model, device, task_id, x_train, t_train, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for start in range(0, len(t_train)-1, 100):\n",
    "      end = start + 100\n",
    "      x, y = torch.from_numpy(x_train[start:end]), torch.from_numpy(t_train[start:end]).long()\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      output = model(x)\n",
    "      loss = F.cross_entropy(output, y)\n",
    "      \n",
    "      ### magic here! :-)\n",
    "      for task in range(task_id):\n",
    "        for name, param in model.named_parameters():\n",
    "          fisher = fisher_dict[task][name]\n",
    "          optpar = optpar_dict[task][name]\n",
    "          loss += (fisher * (optpar - param).pow(2)).sum() * ewc_lambda\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      #print(loss.item())\n",
    "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on task:  0\n",
      "Train Epoch: 1 \tLoss: 2.312119\n",
      "Train Epoch: 2 \tLoss: 2.310108\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[1;32m      9\u001b[0m   train_ewc(trial_flava, device, \u001b[39mid\u001b[39m, x_train_rgb, t_train, optimizer, epoch)\n\u001b[0;32m---> 10\u001b[0m on_task_update(\u001b[39mid\u001b[39;49m, x_train_rgb, t_train)\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m id_test, task \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tasks):\n\u001b[1;32m     13\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTesting on task: \u001b[39m\u001b[39m\"\u001b[39m, id_test)\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36mon_task_update\u001b[0;34m(task_id, x_mem, t_mem)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_task_update\u001b[39m(task_id, x_mem, t_mem):\n\u001b[0;32m----> 3\u001b[0m   model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m   optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      6\u001b[0m   \u001b[39m# accumulating gradients\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ewc_accs = []\n",
    "for id, task in enumerate(tasks):\n",
    "  avg_acc = 0\n",
    "  print(\"Training on task: \", id)\n",
    "  \n",
    "  (x_train_rgb, t_train), _ = task\n",
    "  \n",
    "  for epoch in range(1, 3):\n",
    "    train_ewc(trial_flava, device, id, x_train_rgb, t_train, optimizer, epoch)\n",
    "  on_task_update(id, x_train_rgb, t_train)\n",
    "    \n",
    "  for id_test, task in enumerate(tasks):\n",
    "    print(\"Testing on task: \", id_test)\n",
    "    _, (x_test_rgb, t_test) = task\n",
    "    acc = test_new(trial_flava, device, x_test_rgb, t_test)\n",
    "    avg_acc = avg_acc + acc\n",
    "   \n",
    "  print(\"Avg acc: \", avg_acc / 3)\n",
    "  ewc_accs.append(avg_acc / 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
